{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_v2 import build_lm_classifier_inference, LSTM_SAVED_STATE\n",
    "from utils import get_batch_classifier_inference, clean_text\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intent_102/checkpoints/model_configs.json', 'r') as inp:\n",
    "    lm_params = json.load(inp)\n",
    "with open('intent_102/checkpoints/classifier_configs.json', 'r') as inp:\n",
    "    cls_params = json.load(inp)\n",
    "# Load in CPU\n",
    "language_model, classifier = build_lm_classifier_inference(lm_params, cls_params, is_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from intent_102/checkpoints/classifier_interest_cpu/model.cpkt-150\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "saver = tf.train.Saver([x for x in tf.global_variables() if x not in tf.get_collection(LSTM_SAVED_STATE)])\n",
    "saver.restore(session, 'intent_102/checkpoints/classifier_interest_cpu/model.cpkt-150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('IntentDetection/word2idx.json', 'r') as inp:\n",
    "    word2idx = json.load(inp)\n",
    "with open('IntentDetection/char2idx.json', 'r') as inp:\n",
    "    char2idx = json.load(inp)\n",
    "word2char = {w: [char2idx[c] for c in w] for w in word2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(texts, bsz=32):\n",
    "    texts = [clean_text(x.strip()) for x in texts]\n",
    "    texts = np.array([[word2char.get(w, word2char['<UNK>']) for w in sent.split()] for sent in texts])\n",
    "    results = []\n",
    "    for chars, lens, char_lens in get_batch_classifier_inference(texts, bsz):\n",
    "        probs = session.run(classifier.probs, feed_dict={\n",
    "            language_model.inputs: chars, language_model.seq_lens: lens,\n",
    "            language_model.char_lens: char_lens, language_model.bptt: 20\n",
    "        })\n",
    "        results.append(probs)\n",
    "    return np.concatenate(results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('IntentDetection/total_purchase_text - totalpurchase_text.csv', names=['label', 'mention', '1', '2'], skiprows=1)[['label', 'mention']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data['interest'] = data['label'].map(lambda x: int('I' in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pred'] = np.argmax(predict(data['mention']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
