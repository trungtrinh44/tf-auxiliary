{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from elmo import elmo_embedding\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from model_v2 import LanguageModel\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ner2idx.json', 'r') as inp:\n",
    "    ner2idx = json.load(inp)\n",
    "idx2ner = {v: k for k, v in ner2idx.items()}\n",
    "with open('baomoi_punc/word2idx.json', 'r') as inp:\n",
    "    word2idx = json.load(inp)\n",
    "with open('baomoi_punc/char2idx.json', 'r') as inp:\n",
    "    char2idx = json.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(inputs, labels, seq_lens, n_units, n_classes, drop_i, drop_o, name='tagger', reuse=False, is_training=True):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        def __cell():\n",
    "            cell = tf.contrib.rnn.GRUBlockCellV2(n_units, name='cell', reuse=reuse)\n",
    "            if is_training:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, input_keep_prob=1.0-drop_i, output_keep_prob=1.0-drop_o, variational_recurrent=True, input_size=inputs.shape[-1], dtype=tf.float32)\n",
    "            return cell\n",
    "        outputs, state = tf.nn.bidirectional_dynamic_rnn(__cell(), __cell(), inputs, seq_lens, time_major=False, dtype=tf.float32)\n",
    "        outputs = tf.concat(outputs + (inputs,), axis=-1)\n",
    "        s = tf.shape(outputs)\n",
    "        W = tf.get_variable(name='W', shape=(outputs.shape[2], n_classes), initializer=tf.glorot_uniform_initializer(), trainable=True)\n",
    "        b = tf.get_variable(name='b', shape=(n_classes, ), initializer=tf.zeros_initializer(), trainable=True)\n",
    "        outputs = tf.reshape(outputs, (s[0] * s[1], s[2]), name='before_proj')\n",
    "        outputs = tf.nn.xw_plus_b(outputs, W, b)\n",
    "        outputs = tf.reshape(outputs, (s[0], s[1], n_classes), name='after_proj')\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n",
    "            inputs=outputs,\n",
    "            tag_indices=labels,\n",
    "            sequence_lengths=seq_lens\n",
    "        )\n",
    "        loss = tf.reduce_mean(-log_likelihood)\n",
    "        decode_tags, best_scores = tf.contrib.crf.crf_decode(\n",
    "            potentials=outputs,\n",
    "            transition_params=transition_params,\n",
    "            sequence_length=seq_lens\n",
    "        )\n",
    "        mask = tf.sequence_mask(seq_lens, dtype=tf.float32)\n",
    "        acc = tf.reduce_sum(tf.to_float(tf.equal(decode_tags, labels)) * mask) / tf.reduce_sum(mask)\n",
    "    return outputs, loss, transition_params, decode_tags, best_scores, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda2/envs/tf19/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /root/anaconda2/envs/tf19/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "session = tf.Session()\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, None, 1024, 4), name='x')\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None, None), name='y')\n",
    "seq_len = tf.placeholder(dtype=tf.int32, shape=(None,), name='seq_len')\n",
    "drop_i = tf.placeholder(dtype=tf.float32, shape=(), name='drop_i')\n",
    "drop_o = tf.placeholder(dtype=tf.float32, shape=(), name='drop_o')\n",
    "elmo, elmo_l2_reg = elmo_embedding(x, seq_len)\n",
    "outputs, loss, transition_params, decode_tags, best_scores, acc = tagger(elmo, y, seq_len, 200, 22, drop_i, drop_o)\n",
    "tag_saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = str(unicodedata.normalize('NFKC', x.lower()))\n",
    "    return re.sub('\\d+','N', re.sub('[ ]+',' ', re.sub('[\\n\\r][ \\n\\r]*',' L ', re.sub(r'(?P<punc>\\W)',' \\g<punc> ', x))))\n",
    "\n",
    "def pad_sequence(words):\n",
    "    maxlen = max(len(x) for x in words)\n",
    "    arr = np.zeros(shape=(len(words), 1, maxlen))\n",
    "    for ir in range(len(arr)):\n",
    "        s = words[ir]\n",
    "        arr[ir][0][:len(s)] = s\n",
    "    return arr\n",
    "\n",
    "def __embed_sequence(sentence):\n",
    "    unk_char_idx = char2idx['U']\n",
    "    sentence = [[char2idx.get(x, unk_char_idx) for x in word] for word in sentence]\n",
    "    seq_len = len(sentence)\n",
    "    inputs = pad_sequence(sentence)\n",
    "    embeddings = session.run(lm_model.concated_timewise_output, feed_dict={\n",
    "        lm_model.inputs: inputs, lm_model.seq_lens: [seq_len], lm_model.reset_state: True\n",
    "    })\n",
    "    return embeddings\n",
    "def embed_sentence(sentence):\n",
    "#     sentence = clean_text(sentence).split()\n",
    "    return __embed_sequence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from 15/checkpoints/test/model.cpkt-315616\n",
      "INFO:tensorflow:Restoring parameters from ./ner_tagger_2.cpkt-994\n"
     ]
    }
   ],
   "source": [
    "with open('15/checkpoints/model_configs.json', 'r') as inp:\n",
    "    params = json.load(inp)\n",
    "\n",
    "lm_model = LanguageModel(**params, is_training=False, is_encoding=True)\n",
    "\n",
    "lm_model.build_model()\n",
    "lm_saver = tf.train.Saver([x for x in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'LanguageModel') if x not in tf.get_collection('LSTM_SAVED_STATE')])\n",
    "session.run(tf.global_variables_initializer())\n",
    "lm_saver.restore(session, '15/checkpoints/test/model.cpkt-315616')\n",
    "tag_saver.restore(session, './ner_tagger_2.cpkt-994')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tag(sentence):\n",
    "    sentence = clean_text(sentence).split()\n",
    "    em = np.transpose(embed_sentence(sentence), (1, 0, 2, 3))\n",
    "    tags = session.run(decode_tags, feed_dict={\n",
    "        x: em, seq_len: [len(em[0])], drop_i: 0.0, drop_o: 0.0\n",
    "    })\n",
    "    return [(w, idx2ner[t]) for w, t in zip(sentence, tags[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('đường', 'O'),\n",
       " ('tăng', 'O'),\n",
       " ('đi', 'O'),\n",
       " ('đến', 'O'),\n",
       " ('tây', 'B-LOC'),\n",
       " ('thiên', 'I-LOC'),\n",
       " ('để', 'O'),\n",
       " ('thỉnh', 'O'),\n",
       " ('kinh', 'O')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag(\"\"\"d đi đến Tây Thiên để thỉnh kinh\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chào', 'O'),\n",
       " ('mọi', 'O'),\n",
       " ('người', 'O'),\n",
       " (',', 'O'),\n",
       " ('mình', 'O'),\n",
       " ('hiện', 'O'),\n",
       " ('đang', 'O'),\n",
       " ('làm', 'O'),\n",
       " ('việc', 'O'),\n",
       " ('trong', 'O'),\n",
       " ('group', 'B-ORG'),\n",
       " ('caffeN', 'O'),\n",
       " ('/', 'O'),\n",
       " ('pytorch', 'O'),\n",
       " ('(', 'O'),\n",
       " ('https', 'O'),\n",
       " (':', 'O'),\n",
       " ('/', 'O'),\n",
       " ('/', 'O'),\n",
       " ('pytorch', 'O'),\n",
       " ('.', 'O'),\n",
       " ('org', 'O'),\n",
       " ('/', 'O'),\n",
       " (')', 'O'),\n",
       " (',', 'O'),\n",
       " ('ai', 'O'),\n",
       " ('deep', 'O'),\n",
       " ('learning', 'O'),\n",
       " ('platform', 'O'),\n",
       " ('của', 'O'),\n",
       " ('facebook', 'B-ORG'),\n",
       " ('.', 'O'),\n",
       " ('hiện', 'O'),\n",
       " ('nay', 'O'),\n",
       " (',', 'O'),\n",
       " ('facebook', 'O'),\n",
       " ('là', 'O'),\n",
       " ('công', 'O'),\n",
       " ('ty', 'O'),\n",
       " ('đang', 'O'),\n",
       " ('giải', 'O'),\n",
       " ('quyết', 'O'),\n",
       " ('nhiều', 'O'),\n",
       " ('vấn', 'O'),\n",
       " ('đề', 'O'),\n",
       " ('thú', 'O'),\n",
       " ('vị', 'O'),\n",
       " ('về', 'O'),\n",
       " ('ai', 'O'),\n",
       " ('.', 'O'),\n",
       " ('nếu', 'O'),\n",
       " ('bạn', 'O'),\n",
       " ('nào', 'O'),\n",
       " ('quan', 'O'),\n",
       " ('tâm', 'O'),\n",
       " ('đến', 'O'),\n",
       " ('công', 'O'),\n",
       " ('việc', 'O'),\n",
       " ('/', 'O'),\n",
       " ('thực', 'O'),\n",
       " ('tập', 'O'),\n",
       " ('về', 'O'),\n",
       " ('ai', 'O'),\n",
       " ('/', 'O'),\n",
       " ('deep', 'O'),\n",
       " ('learning', 'O'),\n",
       " ('(', 'O'),\n",
       " ('hay', 'O'),\n",
       " ('engineering', 'O'),\n",
       " ('nói', 'O'),\n",
       " ('chung', 'O'),\n",
       " (')', 'O'),\n",
       " ('tại', 'O'),\n",
       " ('facebook', 'O'),\n",
       " ('thì', 'O'),\n",
       " ('có', 'O'),\n",
       " ('thể', 'O'),\n",
       " ('liên', 'O'),\n",
       " ('hệ', 'O'),\n",
       " ('với', 'O'),\n",
       " ('mình', 'O'),\n",
       " ('nhé', 'O'),\n",
       " ('.', 'O'),\n",
       " ('chi', 'O'),\n",
       " ('tiết', 'O'),\n",
       " ('https', 'O'),\n",
       " (':', 'O'),\n",
       " ('/', 'O'),\n",
       " ('/', 'O'),\n",
       " ('github', 'O'),\n",
       " ('.', 'O'),\n",
       " ('com', 'O'),\n",
       " ('/', 'O'),\n",
       " ('.', 'O'),\n",
       " ('.', 'O'),\n",
       " ('.', 'O'),\n",
       " ('/', 'O'),\n",
       " ('bl', 'B-ORG'),\n",
       " ('.', 'O'),\n",
       " ('.', 'O'),\n",
       " ('.', 'O'),\n",
       " ('/', 'O'),\n",
       " ('master', 'O'),\n",
       " ('/', 'O'),\n",
       " ('interview', 'O'),\n",
       " ('/', 'O'),\n",
       " ('co', 'O'),\n",
       " ('-', 'O'),\n",
       " ('hoi', 'O'),\n",
       " ('-', 'O'),\n",
       " ('facebook', 'O'),\n",
       " ('.', 'O'),\n",
       " ('md', 'B-ORG')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag(\"\"\"Chào mọi người, mình hiện đang làm việc trong group Caffe2/Pytorch (https://pytorch.org/), AI deep learning platform của facebook. Hiện nay, Facebook là công ty đang giải quyết nhiều vấn đề thú vị về AI. Nếu bạn nào quan tâm đến công việc / thực tập về AI / deep learning (hay engineering nói chung) tại Facebook thì có thể liên hệ với mình nhé. chi tiết https://github.com/…/bl…/master/interview/co-hoi-facebook.md\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'O'),\n",
       " ('/', 'O'),\n",
       " ('c', 'O'),\n",
       " ('nào', 'O'),\n",
       " ('dư', 'O'),\n",
       " ('cuốn', 'O'),\n",
       " ('phương', 'O'),\n",
       " ('pháp', 'O'),\n",
       " ('luận', 'O'),\n",
       " ('nghiên', 'O'),\n",
       " ('cứu', 'O'),\n",
       " ('khoa', 'O'),\n",
       " ('học', 'O'),\n",
       " ('của', 'O'),\n",
       " ('vũ', 'B-PER'),\n",
       " ('cao', 'I-PER'),\n",
       " ('đàm', 'I-PER'),\n",
       " ('xuất', 'O'),\n",
       " ('bản', 'O'),\n",
       " ('N', 'O'),\n",
       " ('sách', 'O'),\n",
       " ('gốc', 'O'),\n",
       " ('ko', 'O'),\n",
       " ('ạ', 'O'),\n",
       " ('.', 'O'),\n",
       " ('có', 'O'),\n",
       " ('cho', 'O'),\n",
       " ('e', 'B-PER'),\n",
       " ('mua', 'O'),\n",
       " ('lại', 'O'),\n",
       " ('thank', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag(\"\"\"A/c nào dư cuốn phương pháp luận nghiên cứu khoa học của vũ cao đàm xuất bản 2014 sách gốc ko ạ.Có cho e mua lại thank.\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
