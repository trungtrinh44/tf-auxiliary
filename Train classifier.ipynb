{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('class2idx.json', 'r') as inp:\n",
    "    class2idx = json.load(inp)\n",
    "with open('word2idx.json', 'r') as inp:\n",
    "    word2idx = json.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('train.npy')\n",
    "valid_data = np.load('valid.npy')\n",
    "test_data = np.load('test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer params {'model_configs': {'rnn_layers': [{'units': 1200, 'input_size': 400, 'drop_i': 0.01, 'drop_w': 0.1, 'drop_o': 0.01}, {'units': 1200, 'input_size': 1200, 'drop_w': 0.1, 'drop_o': 0.01}, {'units': 400, 'input_size': 1200, 'drop_o': 0.2, 'drop_w': 0.1}], 'vocab_size': 380, 'drop_e': 0.0}, 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'decay_freq': 100000, 'decay_rate': 0.9, 'wdecay': 1.2e-06, 'alpha': 0.0, 'beta': 0.0, 'clip_norm': 1.0, 'bptt': 200, 'use_ema': True, 'save_freq': 1000, 'log_path': '5/logs', 'train_summary_dir': '5/train_summary/', 'test_summary_dir': '5/test_summary/', 'checkpoint_dir': '5/checkpoints/'}\n",
      "[<tf.Variable 'LanguageModel/embedding_weight:0' shape=(380, 400) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell/W:0' shape=(400, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell/U:0' shape=(1200, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell/bias:0' shape=(4800,) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_1/W:0' shape=(1200, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_1/U:0' shape=(1200, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_1/bias:0' shape=(4800,) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_2/W:0' shape=(1200, 1600) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_2/U:0' shape=(400, 1600) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_2/bias:0' shape=(1600,) dtype=float32_ref>, <tf.Variable 'LanguageModel/decoder_b:0' shape=(380,) dtype=float32_ref>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from 5/checkpoints/train/model.cpkt-111224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from 5/checkpoints/train/model.cpkt-111224\n",
      "Restore variables: [<tf.Variable 'LM_Trainer/global_step:0' shape=() dtype=int32_ref>, <tf.Variable 'LanguageModel/decoder_b/ExponentialMovingAverage:0' shape=(380,) dtype=float32_ref>, <tf.Variable 'LanguageModel/decoder_b:0' shape=(380,) dtype=float32_ref>, <tf.Variable 'LanguageModel/embedding_weight/ExponentialMovingAverage:0' shape=(380, 400) dtype=float32_ref>, <tf.Variable 'LanguageModel/embedding_weight:0' shape=(380, 400) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell/U/ExponentialMovingAverage:0' shape=(1200, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell/U:0' shape=(1200, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell/W/ExponentialMovingAverage:0' shape=(400, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell/W:0' shape=(400, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell/bias/ExponentialMovingAverage:0' shape=(4800,) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell/bias:0' shape=(4800,) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_1/U/ExponentialMovingAverage:0' shape=(1200, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_1/U:0' shape=(1200, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_1/W/ExponentialMovingAverage:0' shape=(1200, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_1/W:0' shape=(1200, 4800) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_1/bias/ExponentialMovingAverage:0' shape=(4800,) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_1/bias:0' shape=(4800,) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_2/U/ExponentialMovingAverage:0' shape=(400, 1600) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_2/U:0' shape=(400, 1600) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_2/W/ExponentialMovingAverage:0' shape=(1200, 1600) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_2/W:0' shape=(1200, 1600) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_2/bias/ExponentialMovingAverage:0' shape=(1600,) dtype=float32_ref>, <tf.Variable 'LanguageModel/lstm_fused_cell_2/bias:0' shape=(1600,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "VERSION = 5\n",
    "params = dict(\n",
    "    model_configs = {\n",
    "   \"rnn_layers\":[\n",
    "          {\n",
    "             \"units\": 1200,\n",
    "             \"input_size\":400,\n",
    "             \"drop_i\": 0.01,\n",
    "             \"drop_w\": 0.1,\n",
    "             \"drop_o\": 0.01\n",
    "          },\n",
    "          {\n",
    "             \"units\": 1200,\n",
    "             \"input_size\": 1200,\n",
    "             \"drop_w\": 0.1,\n",
    "             \"drop_o\": 0.01\n",
    "          },\n",
    "          {\n",
    "             \"units\": 400,\n",
    "             \"input_size\": 1200,\n",
    "             \"drop_o\": 0.2,\n",
    "             \"drop_w\": 0.1\n",
    "          }\n",
    "       ],\n",
    "       \"vocab_size\": len(word2idx),\n",
    "       \"drop_e\": 0.0\n",
    "    },\n",
    "    optimizer = tf.train.AdamOptimizer,\n",
    "    learning_rate = 1e-3,\n",
    "    decay_freq = 100000,\n",
    "    decay_rate = 0.9,\n",
    "    wdecay = 1.2e-6,\n",
    "    alpha = 0.0,\n",
    "    beta = 0.0,\n",
    "    clip_norm = 1.0,\n",
    "    bptt = 200,\n",
    "    use_ema = True,\n",
    "    save_freq = 1000,\n",
    "    log_path = '{}/logs'.format(VERSION),\n",
    "    train_summary_dir = '{}/train_summary/'.format(VERSION),\n",
    "    test_summary_dir = '{}/test_summary/'.format(VERSION),\n",
    "    checkpoint_dir = '{}/checkpoints/'.format(VERSION)\n",
    ")\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "my_trainer = trainer.Trainer(**params)\n",
    "\n",
    "my_trainer.logger.info('Trainer params {}'.format(params))\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "my_trainer.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier configs: {'hiddens': [{'filters': 100, 'kernel_size': 5, 'drop_i': 0.5, 'drop_o': 0.5, 'l2_reg': 0.002}, {'filters': 100, 'kernel_size': 5, 'dilation_rate': 2, 'drop_o': 0.5, 'l2_reg': 0.002}, {'filters': 100, 'kernel_size': 5, 'dilation_rate': 2, 'drop_o': 0.5, 'l2_reg': 0.002}], 'k': 3}\n",
      "['Classifier_0/conv1d_0/kernel:0', 'Classifier_0/conv1d_0/bias:0', 'Classifier_0/batch_normalization/gamma:0', 'Classifier_0/batch_normalization/beta:0', 'Classifier_0/batch_normalization/moving_mean:0', 'Classifier_0/batch_normalization/moving_variance:0', 'Classifier_0/conv1d_1/kernel:0', 'Classifier_0/conv1d_1/bias:0', 'Classifier_0/batch_normalization_1/gamma:0', 'Classifier_0/batch_normalization_1/beta:0', 'Classifier_0/batch_normalization_1/moving_mean:0', 'Classifier_0/batch_normalization_1/moving_variance:0', 'Classifier_0/conv1d_2/kernel:0', 'Classifier_0/conv1d_2/bias:0', 'Classifier_0/batch_normalization_2/gamma:0', 'Classifier_0/batch_normalization_2/beta:0', 'Classifier_0/batch_normalization_2/moving_mean:0', 'Classifier_0/batch_normalization_2/moving_variance:0', 'Classifier_0/dense/kernel:0', 'Classifier_0/dense/bias:0', 'Classifier_1/conv1d_0/kernel:0', 'Classifier_1/conv1d_0/bias:0', 'Classifier_1/batch_normalization/gamma:0', 'Classifier_1/batch_normalization/beta:0', 'Classifier_1/batch_normalization/moving_mean:0', 'Classifier_1/batch_normalization/moving_variance:0', 'Classifier_1/conv1d_1/kernel:0', 'Classifier_1/conv1d_1/bias:0', 'Classifier_1/batch_normalization_1/gamma:0', 'Classifier_1/batch_normalization_1/beta:0', 'Classifier_1/batch_normalization_1/moving_mean:0', 'Classifier_1/batch_normalization_1/moving_variance:0', 'Classifier_1/conv1d_2/kernel:0', 'Classifier_1/conv1d_2/bias:0', 'Classifier_1/batch_normalization_2/gamma:0', 'Classifier_1/batch_normalization_2/beta:0', 'Classifier_1/batch_normalization_2/moving_mean:0', 'Classifier_1/batch_normalization_2/moving_variance:0', 'Classifier_1/dense/kernel:0', 'Classifier_1/dense/bias:0', 'Classifier_2/conv1d_0/kernel:0', 'Classifier_2/conv1d_0/bias:0', 'Classifier_2/batch_normalization/gamma:0', 'Classifier_2/batch_normalization/beta:0', 'Classifier_2/batch_normalization/moving_mean:0', 'Classifier_2/batch_normalization/moving_variance:0', 'Classifier_2/conv1d_1/kernel:0', 'Classifier_2/conv1d_1/bias:0', 'Classifier_2/batch_normalization_1/gamma:0', 'Classifier_2/batch_normalization_1/beta:0', 'Classifier_2/batch_normalization_1/moving_mean:0', 'Classifier_2/batch_normalization_1/moving_variance:0', 'Classifier_2/conv1d_2/kernel:0', 'Classifier_2/conv1d_2/bias:0', 'Classifier_2/batch_normalization_2/gamma:0', 'Classifier_2/batch_normalization_2/beta:0', 'Classifier_2/batch_normalization_2/moving_mean:0', 'Classifier_2/batch_normalization_2/moving_variance:0', 'Classifier_2/dense/kernel:0', 'Classifier_2/dense/bias:0', 'Classifier_3/conv1d_0/kernel:0', 'Classifier_3/conv1d_0/bias:0', 'Classifier_3/batch_normalization/gamma:0', 'Classifier_3/batch_normalization/beta:0', 'Classifier_3/batch_normalization/moving_mean:0', 'Classifier_3/batch_normalization/moving_variance:0', 'Classifier_3/conv1d_1/kernel:0', 'Classifier_3/conv1d_1/bias:0', 'Classifier_3/batch_normalization_1/gamma:0', 'Classifier_3/batch_normalization_1/beta:0', 'Classifier_3/batch_normalization_1/moving_mean:0', 'Classifier_3/batch_normalization_1/moving_variance:0', 'Classifier_3/conv1d_2/kernel:0', 'Classifier_3/conv1d_2/bias:0', 'Classifier_3/batch_normalization_2/gamma:0', 'Classifier_3/batch_normalization_2/beta:0', 'Classifier_3/batch_normalization_2/moving_mean:0', 'Classifier_3/batch_normalization_2/moving_variance:0', 'Classifier_3/dense/kernel:0', 'Classifier_3/dense/bias:0', 'Classifier_4/conv1d_0/kernel:0', 'Classifier_4/conv1d_0/bias:0', 'Classifier_4/batch_normalization/gamma:0', 'Classifier_4/batch_normalization/beta:0', 'Classifier_4/batch_normalization/moving_mean:0', 'Classifier_4/batch_normalization/moving_variance:0', 'Classifier_4/conv1d_1/kernel:0', 'Classifier_4/conv1d_1/bias:0', 'Classifier_4/batch_normalization_1/gamma:0', 'Classifier_4/batch_normalization_1/beta:0', 'Classifier_4/batch_normalization_1/moving_mean:0', 'Classifier_4/batch_normalization_1/moving_variance:0', 'Classifier_4/conv1d_2/kernel:0', 'Classifier_4/conv1d_2/bias:0', 'Classifier_4/batch_normalization_2/gamma:0', 'Classifier_4/batch_normalization_2/beta:0', 'Classifier_4/batch_normalization_2/moving_mean:0', 'Classifier_4/batch_normalization_2/moving_variance:0', 'Classifier_4/dense/kernel:0', 'Classifier_4/dense/bias:0', 'class_global_step:0', 'Classifier_0/conv1d_0/kernel/Adam:0', 'Classifier_0/conv1d_0/kernel/Adam_1:0', 'Classifier_0/conv1d_0/bias/Adam:0', 'Classifier_0/conv1d_0/bias/Adam_1:0', 'Classifier_0/batch_normalization/gamma/Adam:0', 'Classifier_0/batch_normalization/gamma/Adam_1:0', 'Classifier_0/batch_normalization/beta/Adam:0', 'Classifier_0/batch_normalization/beta/Adam_1:0', 'Classifier_0/conv1d_1/kernel/Adam:0', 'Classifier_0/conv1d_1/kernel/Adam_1:0', 'Classifier_0/conv1d_1/bias/Adam:0', 'Classifier_0/conv1d_1/bias/Adam_1:0', 'Classifier_0/batch_normalization_1/gamma/Adam:0', 'Classifier_0/batch_normalization_1/gamma/Adam_1:0', 'Classifier_0/batch_normalization_1/beta/Adam:0', 'Classifier_0/batch_normalization_1/beta/Adam_1:0', 'Classifier_0/conv1d_2/kernel/Adam:0', 'Classifier_0/conv1d_2/kernel/Adam_1:0', 'Classifier_0/conv1d_2/bias/Adam:0', 'Classifier_0/conv1d_2/bias/Adam_1:0', 'Classifier_0/batch_normalization_2/gamma/Adam:0', 'Classifier_0/batch_normalization_2/gamma/Adam_1:0', 'Classifier_0/batch_normalization_2/beta/Adam:0', 'Classifier_0/batch_normalization_2/beta/Adam_1:0', 'Classifier_0/dense/kernel/Adam:0', 'Classifier_0/dense/kernel/Adam_1:0', 'Classifier_0/dense/bias/Adam:0', 'Classifier_0/dense/bias/Adam_1:0', 'Classifier_1/conv1d_0/kernel/Adam:0', 'Classifier_1/conv1d_0/kernel/Adam_1:0', 'Classifier_1/conv1d_0/bias/Adam:0', 'Classifier_1/conv1d_0/bias/Adam_1:0', 'Classifier_1/batch_normalization/gamma/Adam:0', 'Classifier_1/batch_normalization/gamma/Adam_1:0', 'Classifier_1/batch_normalization/beta/Adam:0', 'Classifier_1/batch_normalization/beta/Adam_1:0', 'Classifier_1/conv1d_1/kernel/Adam:0', 'Classifier_1/conv1d_1/kernel/Adam_1:0', 'Classifier_1/conv1d_1/bias/Adam:0', 'Classifier_1/conv1d_1/bias/Adam_1:0', 'Classifier_1/batch_normalization_1/gamma/Adam:0', 'Classifier_1/batch_normalization_1/gamma/Adam_1:0', 'Classifier_1/batch_normalization_1/beta/Adam:0', 'Classifier_1/batch_normalization_1/beta/Adam_1:0', 'Classifier_1/conv1d_2/kernel/Adam:0', 'Classifier_1/conv1d_2/kernel/Adam_1:0', 'Classifier_1/conv1d_2/bias/Adam:0', 'Classifier_1/conv1d_2/bias/Adam_1:0', 'Classifier_1/batch_normalization_2/gamma/Adam:0', 'Classifier_1/batch_normalization_2/gamma/Adam_1:0', 'Classifier_1/batch_normalization_2/beta/Adam:0', 'Classifier_1/batch_normalization_2/beta/Adam_1:0', 'Classifier_1/dense/kernel/Adam:0', 'Classifier_1/dense/kernel/Adam_1:0', 'Classifier_1/dense/bias/Adam:0', 'Classifier_1/dense/bias/Adam_1:0', 'Classifier_2/conv1d_0/kernel/Adam:0', 'Classifier_2/conv1d_0/kernel/Adam_1:0', 'Classifier_2/conv1d_0/bias/Adam:0', 'Classifier_2/conv1d_0/bias/Adam_1:0', 'Classifier_2/batch_normalization/gamma/Adam:0', 'Classifier_2/batch_normalization/gamma/Adam_1:0', 'Classifier_2/batch_normalization/beta/Adam:0', 'Classifier_2/batch_normalization/beta/Adam_1:0', 'Classifier_2/conv1d_1/kernel/Adam:0', 'Classifier_2/conv1d_1/kernel/Adam_1:0', 'Classifier_2/conv1d_1/bias/Adam:0', 'Classifier_2/conv1d_1/bias/Adam_1:0', 'Classifier_2/batch_normalization_1/gamma/Adam:0', 'Classifier_2/batch_normalization_1/gamma/Adam_1:0', 'Classifier_2/batch_normalization_1/beta/Adam:0', 'Classifier_2/batch_normalization_1/beta/Adam_1:0', 'Classifier_2/conv1d_2/kernel/Adam:0', 'Classifier_2/conv1d_2/kernel/Adam_1:0', 'Classifier_2/conv1d_2/bias/Adam:0', 'Classifier_2/conv1d_2/bias/Adam_1:0', 'Classifier_2/batch_normalization_2/gamma/Adam:0', 'Classifier_2/batch_normalization_2/gamma/Adam_1:0', 'Classifier_2/batch_normalization_2/beta/Adam:0', 'Classifier_2/batch_normalization_2/beta/Adam_1:0', 'Classifier_2/dense/kernel/Adam:0', 'Classifier_2/dense/kernel/Adam_1:0', 'Classifier_2/dense/bias/Adam:0', 'Classifier_2/dense/bias/Adam_1:0', 'Classifier_3/conv1d_0/kernel/Adam:0', 'Classifier_3/conv1d_0/kernel/Adam_1:0', 'Classifier_3/conv1d_0/bias/Adam:0', 'Classifier_3/conv1d_0/bias/Adam_1:0', 'Classifier_3/batch_normalization/gamma/Adam:0', 'Classifier_3/batch_normalization/gamma/Adam_1:0', 'Classifier_3/batch_normalization/beta/Adam:0', 'Classifier_3/batch_normalization/beta/Adam_1:0', 'Classifier_3/conv1d_1/kernel/Adam:0', 'Classifier_3/conv1d_1/kernel/Adam_1:0', 'Classifier_3/conv1d_1/bias/Adam:0', 'Classifier_3/conv1d_1/bias/Adam_1:0', 'Classifier_3/batch_normalization_1/gamma/Adam:0', 'Classifier_3/batch_normalization_1/gamma/Adam_1:0', 'Classifier_3/batch_normalization_1/beta/Adam:0', 'Classifier_3/batch_normalization_1/beta/Adam_1:0', 'Classifier_3/conv1d_2/kernel/Adam:0', 'Classifier_3/conv1d_2/kernel/Adam_1:0', 'Classifier_3/conv1d_2/bias/Adam:0', 'Classifier_3/conv1d_2/bias/Adam_1:0', 'Classifier_3/batch_normalization_2/gamma/Adam:0', 'Classifier_3/batch_normalization_2/gamma/Adam_1:0', 'Classifier_3/batch_normalization_2/beta/Adam:0', 'Classifier_3/batch_normalization_2/beta/Adam_1:0', 'Classifier_3/dense/kernel/Adam:0', 'Classifier_3/dense/kernel/Adam_1:0', 'Classifier_3/dense/bias/Adam:0', 'Classifier_3/dense/bias/Adam_1:0', 'Classifier_4/conv1d_0/kernel/Adam:0', 'Classifier_4/conv1d_0/kernel/Adam_1:0', 'Classifier_4/conv1d_0/bias/Adam:0', 'Classifier_4/conv1d_0/bias/Adam_1:0', 'Classifier_4/batch_normalization/gamma/Adam:0', 'Classifier_4/batch_normalization/gamma/Adam_1:0', 'Classifier_4/batch_normalization/beta/Adam:0', 'Classifier_4/batch_normalization/beta/Adam_1:0', 'Classifier_4/conv1d_1/kernel/Adam:0', 'Classifier_4/conv1d_1/kernel/Adam_1:0', 'Classifier_4/conv1d_1/bias/Adam:0', 'Classifier_4/conv1d_1/bias/Adam_1:0', 'Classifier_4/batch_normalization_1/gamma/Adam:0', 'Classifier_4/batch_normalization_1/gamma/Adam_1:0', 'Classifier_4/batch_normalization_1/beta/Adam:0', 'Classifier_4/batch_normalization_1/beta/Adam_1:0', 'Classifier_4/conv1d_2/kernel/Adam:0', 'Classifier_4/conv1d_2/kernel/Adam_1:0', 'Classifier_4/conv1d_2/bias/Adam:0', 'Classifier_4/conv1d_2/bias/Adam_1:0', 'Classifier_4/batch_normalization_2/gamma/Adam:0', 'Classifier_4/batch_normalization_2/gamma/Adam_1:0', 'Classifier_4/batch_normalization_2/beta/Adam:0', 'Classifier_4/batch_normalization_2/beta/Adam_1:0', 'Classifier_4/dense/kernel/Adam:0', 'Classifier_4/dense/kernel/Adam_1:0', 'Classifier_4/dense/bias/Adam:0', 'Classifier_4/dense/bias/Adam_1:0']\n"
     ]
    }
   ],
   "source": [
    "classifier_configs = dict(\n",
    "    hiddens = [\n",
    "        dict(\n",
    "            filters=100,\n",
    "            kernel_size=5,\n",
    "            drop_i=0.5,\n",
    "            drop_o=0.5,\n",
    "            l2_reg=2e-3\n",
    "        ),\n",
    "        dict(\n",
    "            filters=100,\n",
    "            kernel_size=5,\n",
    "            dilation_rate=2,\n",
    "            drop_o=0.5,\n",
    "            l2_reg=2e-3\n",
    "        ),\n",
    "        dict(\n",
    "            filters=100,\n",
    "            kernel_size=5,\n",
    "            dilation_rate=2,\n",
    "            drop_o=0.5,\n",
    "            l2_reg=2e-3\n",
    "        )\n",
    "    ], \n",
    "    k = 3\n",
    ")\n",
    "my_trainer.add_classifier(num_classes=len(class2idx), classifier_configs=classifier_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, bsz, num_class, noise=0.0, shuffle=True, max_length = 1000000000000):\n",
    "    if shuffle:\n",
    "        data = np.random.permutation(data)\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        next_bsz = min(bsz, len(data) - i)\n",
    "        next_batch = data[i:i+next_bsz]\n",
    "        seq_lens = np.array([min(max_length, len(x)) for x, _ in next_batch], dtype=np.int32)\n",
    "        next_x = np.zeros(shape=(next_bsz, max(seq_lens)), dtype=np.int32)\n",
    "        for j, ((x, _), xl) in enumerate(zip(next_batch, seq_lens)):\n",
    "            next_x[j, :xl] = x[:xl]\n",
    "        next_x = np.transpose(next_x, axes=[1, 0])\n",
    "        next_y = [\n",
    "            np.zeros(shape=(next_bsz, 2), dtype=np.float32) for _ in range(num_class)\n",
    "        ]\n",
    "        for j, (_, c) in enumerate(next_batch):\n",
    "            for ci in range(num_class):\n",
    "                if ci == c:\n",
    "                    next_y[ci][j][1] = 1 - noise\n",
    "                    next_y[ci][j][0] = noise\n",
    "                else:\n",
    "                    next_y[ci][j][0] = 1 - noise\n",
    "                    next_y[ci][j][1] = noise\n",
    "        yield next_x, next_y, seq_lens\n",
    "        i += next_bsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step    1: loss 12.81320, time 03.28\n",
      "Step    2: loss 9.03220, time 00.77\n",
      "Step    3: loss 9.57012, time 00.77\n",
      "Step    4: loss 8.38230, time 00.77\n",
      "Step    5: loss 7.64284, time 00.77\n",
      "Step    6: loss 8.08059, time 00.77\n",
      "Step    7: loss 6.98950, time 00.77\n",
      "Step    8: loss 8.06201, time 00.77\n",
      "Step    9: loss 7.34099, time 00.77\n",
      "Step   10: loss 7.73129, time 00.77\n",
      "Step   11: loss 7.59348, time 00.78\n",
      "Step   12: loss 7.73335, time 00.79\n",
      "Step   13: loss 6.44204, time 00.79\n",
      "Step   14: loss 7.96272, time 00.79\n",
      "Step   15: loss 7.68232, time 00.78\n",
      "Step   16: loss 6.52353, time 00.78\n",
      "Step   17: loss 6.92045, time 00.77\n",
      "Step   18: loss 6.08749, time 00.79\n",
      "Step   19: loss 7.18192, time 00.79\n",
      "Step   20: loss 6.27559, time 00.79\n",
      "Step   21: loss 7.19957, time 00.79\n",
      "Step   22: loss 6.29389, time 00.78\n",
      "Step   23: loss 6.10088, time 00.80\n",
      "Step   24: loss 5.71332, time 00.79\n",
      "Step   25: loss 5.91462, time 00.79\n",
      "Step   26: loss 6.54303, time 00.82\n",
      "Step   27: loss 5.98207, time 00.79\n",
      "Step   28: loss 5.40517, time 00.79\n",
      "Step   29: loss 5.61226, time 00.78\n",
      "Step   30: loss 6.58868, time 00.78\n",
      "Step   31: loss 5.55823, time 00.81\n",
      "Step   32: loss 5.61571, time 00.79\n",
      "Step   33: loss 5.74462, time 00.81\n",
      "Step   34: loss 5.33139, time 00.78\n",
      "Step   35: loss 4.59789, time 00.82\n",
      "Step   36: loss 4.71429, time 00.79\n",
      "Step   37: loss 4.34930, time 00.80\n",
      "Step   38: loss 4.75289, time 00.80\n",
      "Step   39: loss 4.50830, time 00.79\n",
      "Step   40: loss 4.15270, time 00.81\n",
      "Step   41: loss 4.97538, time 00.81\n",
      "Step   42: loss 4.85916, time 00.79\n",
      "Step   43: loss 4.38875, time 00.79\n",
      "Step   44: loss 4.56608, time 00.79\n",
      "Step   45: loss 4.26364, time 00.81\n",
      "Step   46: loss 4.30965, time 00.80\n",
      "Step   47: loss 3.72162, time 00.79\n",
      "Step   48: loss 4.01619, time 00.80\n",
      "Step   49: loss 4.00907, time 00.80\n",
      "Step   50: loss 3.72626, time 00.79\n",
      "Step   51: loss 3.27212, time 00.80\n",
      "Step   52: loss 4.26743, time 00.78\n",
      "Step   53: loss 3.80889, time 00.79\n",
      "Step   54: loss 3.79816, time 00.79\n",
      "Step   55: loss 4.10283, time 00.79\n",
      "Step   56: loss 3.85740, time 00.78\n",
      "Step   57: loss 3.65347, time 00.81\n",
      "Step   58: loss 3.32563, time 00.80\n",
      "Step   59: loss 3.54194, time 00.78\n",
      "Step   60: loss 3.34051, time 00.80\n",
      "Step   61: loss 3.30649, time 00.80\n",
      "Step   62: loss 3.19867, time 00.79\n",
      "Step   63: loss 3.40925, time 00.80\n",
      "Step   64: loss 4.03539, time 00.81\n",
      "Step   65: loss 3.38244, time 00.81\n",
      "Step   66: loss 3.39874, time 00.80\n",
      "Step   67: loss 3.07469, time 00.79\n",
      "Step   68: loss 3.24156, time 00.80\n",
      "Step   69: loss 3.30822, time 00.79\n",
      "Step   70: loss 2.87328, time 00.82\n",
      "Step   71: loss 2.87965, time 00.80\n",
      "Step   72: loss 3.13658, time 00.80\n",
      "Step   73: loss 2.90820, time 00.81\n",
      "Step   74: loss 2.92043, time 00.79\n",
      "Step   75: loss 3.22875, time 00.79\n",
      "Step   76: loss 3.12291, time 00.82\n",
      "Step   77: loss 3.04635, time 00.81\n",
      "Step   78: loss 3.02513, time 00.82\n",
      "Step   79: loss 2.87962, time 00.82\n",
      "Step   80: loss 3.31961, time 00.82\n",
      "Step   81: loss 3.03365, time 00.79\n",
      "Step   82: loss 2.64755, time 00.79\n",
      "Step   83: loss 2.95685, time 00.82\n",
      "Step   84: loss 2.83097, time 00.79\n",
      "Step   85: loss 2.88808, time 00.82\n",
      "Step   86: loss 3.03990, time 00.78\n",
      "Step   87: loss 2.80544, time 00.80\n",
      "Step   88: loss 2.82929, time 00.80\n",
      "Step   89: loss 2.69144, time 00.81\n",
      "Step   90: loss 2.74362, time 00.80\n",
      "Step   91: loss 2.68662, time 00.79\n",
      "Step   92: loss 2.73243, time 00.80\n",
      "Step   93: loss 2.58340, time 00.80\n",
      "Step   94: loss 2.81160, time 00.81\n",
      "Step   95: loss 2.67377, time 00.79\n",
      "Step   96: loss 2.58807, time 00.81\n",
      "Step   97: loss 2.90997, time 00.80\n",
      "Step   98: loss 2.67036, time 00.80\n",
      "Step   99: loss 2.59416, time 00.83\n",
      "Step  100: loss 2.70328, time 00.79\n",
      "Step  101: loss 2.55323, time 00.80\n",
      "Step  102: loss 2.56210, time 00.80\n",
      "Step  103: loss 2.66678, time 00.81\n",
      "Step  104: loss 2.59791, time 00.80\n",
      "Step  105: loss 2.64963, time 00.81\n",
      "Step  106: loss 2.66891, time 00.80\n",
      "Step  107: loss 2.53608, time 00.81\n",
      "Step  108: loss 2.41393, time 00.79\n",
      "Step  109: loss 2.57836, time 00.81\n",
      "Step  110: loss 2.71145, time 00.82\n",
      "Step  111: loss 2.90061, time 00.81\n",
      "Step  112: loss 2.49609, time 00.80\n",
      "Step  113: loss 2.70703, time 00.81\n",
      "Step  114: loss 2.66793, time 00.81\n",
      "Step  115: loss 2.48176, time 00.79\n",
      "Step  116: loss 2.55960, time 00.82\n",
      "Step  117: loss 2.56657, time 00.79\n",
      "Step  118: loss 2.56279, time 00.78\n",
      "Step  119: loss 2.58689, time 00.78\n",
      "Step  120: loss 2.48261, time 00.78\n",
      "Step  121: loss 2.40594, time 00.78\n",
      "Step  122: loss 2.66406, time 00.78\n",
      "Step  123: loss 2.54517, time 00.78\n",
      "Step  124: loss 2.42570, time 00.78\n",
      "Step  125: loss 2.72161, time 00.78\n",
      "Step  126: loss 2.74958, time 00.78\n",
      "Step  127: loss 2.52409, time 00.78\n",
      "Step  128: loss 2.42890, time 00.78\n",
      "Step  129: loss 2.48252, time 00.78\n",
      "Step  130: loss 2.58295, time 00.78\n",
      "Step  131: loss 2.55881, time 00.78\n",
      "Step  132: loss 2.51857, time 00.78\n",
      "Step  133: loss 2.43827, time 00.78\n",
      "Step  134: loss 2.39070, time 00.78\n",
      "Step  135: loss 2.41829, time 00.78\n",
      "Step  136: loss 2.45849, time 00.78\n",
      "Step  137: loss 2.44964, time 00.78\n",
      "Step  138: loss 2.48066, time 00.78\n",
      "Step  139: loss 2.67242, time 00.78\n",
      "Step  140: loss 2.51710, time 00.78\n",
      "Step  141: loss 2.35450, time 00.78\n",
      "Step  142: loss 2.41435, time 00.78\n",
      "Step  143: loss 2.32813, time 00.78\n",
      "Step  144: loss 2.53635, time 00.78\n",
      "Step  145: loss 2.53210, time 00.78\n",
      "Step  146: loss 2.69915, time 00.78\n",
      "Step  147: loss 2.43855, time 00.78\n",
      "Step  148: loss 2.56655, time 00.78\n",
      "Step  149: loss 2.43916, time 00.78\n",
      "Step  150: loss 2.37754, time 00.78\n",
      "Step  151: loss 2.59817, time 00.78\n",
      "Step  152: loss 2.40903, time 00.78\n",
      "Step  153: loss 2.47046, time 00.78\n",
      "Step  154: loss 2.55007, time 00.78\n",
      "Step  155: loss 2.40626, time 00.78\n",
      "Step  156: loss 2.33723, time 00.78\n",
      "Step  157: loss 2.48049, time 00.78\n",
      "Step  158: loss 2.45461, time 00.78\n",
      "Step  159: loss 2.44805, time 00.78\n",
      "Step  160: loss 2.56568, time 00.78\n",
      "Step  161: loss 2.40780, time 00.78\n",
      "Step  162: loss 2.42033, time 00.78\n",
      "Step  163: loss 2.39107, time 00.78\n",
      "Step  164: loss 2.39766, time 00.78\n",
      "Step  165: loss 2.58198, time 00.78\n",
      "Step  166: loss 2.42070, time 00.78\n",
      "Step  167: loss 2.33840, time 00.78\n",
      "Step  168: loss 2.44460, time 00.78\n",
      "Step  169: loss 2.47398, time 00.78\n",
      "Step  170: loss 2.50102, time 00.78\n",
      "Step  171: loss 2.70958, time 00.78\n",
      "Step  172: loss 2.48979, time 00.78\n",
      "Step  173: loss 2.44147, time 00.78\n",
      "Step  174: loss 2.30092, time 00.78\n",
      "Step  175: loss 2.39540, time 00.78\n",
      "Step  176: loss 2.40257, time 00.78\n",
      "Step  177: loss 2.44584, time 00.78\n",
      "Step  178: loss 2.47466, time 00.78\n",
      "Step  179: loss 2.49853, time 00.78\n",
      "Step  180: loss 2.31374, time 00.78\n",
      "Step  181: loss 2.56434, time 00.78\n",
      "Step  182: loss 2.22005, time 00.78\n",
      "Step  183: loss 2.55270, time 00.78\n",
      "Step  184: loss 2.46527, time 00.78\n",
      "Step  185: loss 2.43038, time 00.78\n",
      "Step  186: loss 2.46412, time 00.78\n",
      "Step  187: loss 2.54943, time 00.78\n",
      "Step  188: loss 2.42271, time 00.78\n",
      "Step  189: loss 2.32300, time 00.78\n",
      "Step  190: loss 2.38069, time 00.78\n",
      "Step  191: loss 2.25805, time 00.78\n",
      "Step  192: loss 2.43320, time 00.78\n",
      "Step  193: loss 2.55651, time 00.78\n",
      "Step  194: loss 2.46583, time 00.78\n",
      "Step  195: loss 2.47399, time 00.78\n",
      "Step  196: loss 2.38629, time 00.78\n",
      "Step  197: loss 2.31311, time 00.78\n",
      "Step  198: loss 2.47555, time 00.78\n",
      "Step  199: loss 2.47744, time 00.78\n",
      "Step  200: loss 2.28393, time 00.78\n",
      "Step  201: loss 2.39093, time 00.78\n",
      "Step  202: loss 2.52738, time 00.78\n",
      "Step  203: loss 2.51395, time 00.78\n",
      "Step  204: loss 2.26531, time 00.78\n",
      "Step  205: loss 2.34213, time 00.78\n",
      "Step  206: loss 2.48752, time 00.78\n",
      "Step  207: loss 2.34625, time 00.78\n",
      "Step  208: loss 2.38411, time 00.78\n",
      "Step  209: loss 2.50011, time 00.78\n",
      "Step  210: loss 2.36713, time 00.78\n",
      "Step  211: loss 2.35000, time 00.78\n",
      "Step  212: loss 2.45545, time 00.78\n",
      "Step  213: loss 2.39853, time 00.78\n",
      "Step  214: loss 2.35195, time 00.78\n",
      "Step  215: loss 2.35772, time 00.78\n",
      "Step  216: loss 2.28336, time 00.78\n",
      "Step  217: loss 2.32742, time 00.78\n",
      "Step  218: loss 2.37524, time 00.80\n",
      "Step  219: loss 2.47715, time 00.83\n",
      "Step  220: loss 2.56288, time 00.80\n",
      "Step  221: loss 2.30510, time 00.80\n",
      "Step  222: loss 2.50597, time 00.80\n",
      "Step  223: loss 2.32960, time 00.80\n",
      "Step  224: loss 2.33583, time 00.79\n",
      "Step  225: loss 2.32518, time 00.78\n",
      "Step  226: loss 2.26245, time 00.78\n",
      "Step  227: loss 2.30978, time 00.78\n",
      "Step  228: loss 2.35467, time 00.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step  229: loss 2.32105, time 00.78\n",
      "Step  230: loss 2.34105, time 00.78\n",
      "Step  231: loss 2.34823, time 00.78\n",
      "Step  232: loss 2.35511, time 00.78\n",
      "Step  233: loss 2.29582, time 00.78\n",
      "Step  234: loss 2.41042, time 00.78\n",
      "Step  235: loss 2.23152, time 00.78\n",
      "Step  236: loss 2.32175, time 00.78\n",
      "Step  237: loss 2.43583, time 00.78\n",
      "Step  238: loss 2.33133, time 00.78\n",
      "Step  239: loss 2.26305, time 00.78\n",
      "Step  240: loss 2.32147, time 00.78\n",
      "Step  241: loss 2.45133, time 00.78\n",
      "Step  242: loss 2.39264, time 00.78\n",
      "Step  243: loss 2.43063, time 00.78\n",
      "Step  244: loss 2.32183, time 00.78\n",
      "Step  245: loss 2.28183, time 00.78\n",
      "Step  246: loss 2.28861, time 00.78\n",
      "Step  247: loss 2.45089, time 00.78\n",
      "Step  248: loss 2.35794, time 00.78\n",
      "Step  249: loss 2.29117, time 00.78\n",
      "Step  250: loss 2.16284, time 00.78\n",
      "Step  251: loss 2.34560, time 00.78\n",
      "Step  252: loss 2.20883, time 00.78\n",
      "Step  253: loss 2.31659, time 00.78\n",
      "Step  254: loss 2.48436, time 00.78\n",
      "Step  255: loss 2.40806, time 00.78\n",
      "Step  256: loss 2.14958, time 00.78\n",
      "Step  257: loss 2.24376, time 00.78\n",
      "Step  258: loss 2.42673, time 00.78\n",
      "Step  259: loss 2.13820, time 00.78\n",
      "Step  260: loss 2.35512, time 00.78\n",
      "Step  261: loss 2.11748, time 00.78\n",
      "Step  262: loss 2.28983, time 00.78\n",
      "Step  263: loss 2.43000, time 00.78\n",
      "Step  264: loss 2.25615, time 00.78\n",
      "Step  265: loss 2.31898, time 00.78\n",
      "Step  266: loss 2.38739, time 00.78\n",
      "Step  267: loss 2.11426, time 00.78\n",
      "Step  268: loss 2.34224, time 00.78\n",
      "Step  269: loss 2.31878, time 00.78\n",
      "Step  270: loss 2.25941, time 00.78\n",
      "Step  271: loss 2.42432, time 00.78\n",
      "Step  272: loss 2.33088, time 00.78\n",
      "Step  273: loss 2.47590, time 00.78\n",
      "Step  274: loss 2.26068, time 00.78\n",
      "Step  275: loss 2.35631, time 00.79\n",
      "Step  276: loss 2.27998, time 00.81\n",
      "Step  277: loss 2.31767, time 00.81\n",
      "Step  278: loss 2.36103, time 00.82\n",
      "Step  279: loss 2.39298, time 00.82\n",
      "Step  280: loss 2.30001, time 00.82\n",
      "Step  281: loss 2.48021, time 00.80\n",
      "Step  282: loss 2.39195, time 00.81\n",
      "Step  283: loss 2.35021, time 00.81\n",
      "Step  284: loss 2.33884, time 00.80\n",
      "Step  285: loss 2.40474, time 00.79\n",
      "Step  286: loss 2.29937, time 00.81\n",
      "Step  287: loss 2.26627, time 00.78\n",
      "Step  288: loss 2.34583, time 00.80\n",
      "Step  289: loss 2.42358, time 00.81\n",
      "Step  290: loss 2.55967, time 00.34\n",
      "Evaluate step  290: loss 3.00830, time 01.17\n",
      "Evaluate step  290: loss 3.00678, time 01.85\n",
      "Evaluate step  290: loss 3.03936, time 02.56\n",
      "Evaluate step  290: loss 3.04318, time 03.25\n",
      "Evaluate step  290: loss 3.00844, time 03.92\n",
      "Evaluate step  290: loss 3.04689, time 04.61\n",
      "Evaluate step  290: loss 3.00856, time 05.31\n",
      "Evaluate step  290: loss 3.00784, time 05.99\n",
      "Evaluate step  290: loss 3.03598, time 06.67\n",
      "Evaluate step  290: loss 3.01546, time 07.35\n",
      "Evaluate step  290: loss 3.04383, time 08.03\n",
      "Evaluate step  290: loss 3.01063, time 08.72\n",
      "Evaluate step  290: loss 3.09095, time 09.39\n",
      "Evaluate step  290: loss 2.99778, time 10.09\n",
      "Evaluate step  290: loss 3.03470, time 10.78\n",
      "Evaluate step  290: loss 3.00479, time 11.45\n",
      "Evaluate step  290: loss 3.00360, time 12.15\n",
      "Evaluate step  290: loss 3.00704, time 12.82\n",
      "Evaluate step  290: loss 3.04133, time 13.51\n",
      "Evaluate step  290: loss 3.00403, time 14.19\n",
      "Evaluate step  290: loss 3.01162, time 14.88\n",
      "Evaluate step  290: loss 3.04478, time 15.55\n",
      "Evaluate step  290: loss 3.03834, time 16.25\n",
      "Evaluate step  290: loss 3.04843, time 16.96\n",
      "Evaluate step  290: loss 3.02906, time 17.65\n",
      "Evaluate step  290: loss 3.03261, time 18.33\n",
      "Evaluate step  290: loss 3.01876, time 18.99\n",
      "Evaluate step  290: loss 2.99499, time 19.65\n",
      "Evaluate step  290: loss 3.01075, time 20.32\n",
      "Evaluate step  290: loss 2.99993, time 20.98\n",
      "Evaluate step  290: loss 3.07053, time 21.64\n",
      "Evaluate step  290: loss 3.01248, time 22.31\n",
      "Evaluate step  290: loss 3.01709, time 22.97\n",
      "Evaluate step  290: loss 3.01299, time 23.63\n",
      "Evaluate step  290: loss 3.01651, time 24.30\n",
      "Evaluate step  290: loss 3.02155, time 24.86\n",
      "Evaluate total loss: 3.023330509662628, avg. time: 0.7278953347887311\n",
      "Step  291: loss 2.36747, time 00.79\n",
      "Step  292: loss 2.33901, time 00.78\n",
      "Step  293: loss 2.44791, time 00.78\n",
      "Step  294: loss 2.21955, time 00.78\n",
      "Step  295: loss 2.20543, time 00.78\n",
      "Step  296: loss 2.35241, time 00.78\n",
      "Step  297: loss 2.28792, time 00.78\n",
      "Step  298: loss 2.26296, time 00.78\n",
      "Step  299: loss 2.34828, time 00.78\n",
      "Step  300: loss 2.41331, time 00.78\n",
      "Step  301: loss 2.28270, time 00.78\n",
      "Step  302: loss 2.41360, time 00.78\n",
      "Step  303: loss 2.21681, time 00.78\n",
      "Step  304: loss 2.16596, time 00.78\n",
      "Step  305: loss 2.33481, time 00.78\n",
      "Step  306: loss 2.37308, time 00.78\n",
      "Step  307: loss 2.23290, time 00.78\n",
      "Step  308: loss 2.44075, time 00.78\n",
      "Step  309: loss 2.26950, time 00.78\n",
      "Step  310: loss 2.23816, time 00.78\n",
      "Step  311: loss 2.17966, time 00.78\n",
      "Step  312: loss 2.24354, time 00.78\n",
      "Step  313: loss 2.15861, time 00.78\n",
      "Step  314: loss 2.30681, time 00.78\n",
      "Step  315: loss 2.30358, time 00.78\n",
      "Step  316: loss 2.21889, time 00.79\n",
      "Step  317: loss 2.29679, time 00.79\n",
      "Step  318: loss 2.37617, time 00.80\n",
      "Step  319: loss 2.29962, time 00.78\n",
      "Step  320: loss 2.14582, time 00.78\n",
      "Step  321: loss 2.21641, time 00.78\n",
      "Step  322: loss 2.29394, time 00.78\n",
      "Step  323: loss 2.32353, time 00.78\n",
      "Step  324: loss 2.15573, time 00.78\n",
      "Step  325: loss 2.23570, time 00.78\n",
      "Step  326: loss 2.32091, time 00.79\n",
      "Step  327: loss 2.15957, time 00.80\n",
      "Step  328: loss 2.10742, time 00.79\n",
      "Step  329: loss 2.31666, time 00.79\n",
      "Step  330: loss 2.33428, time 00.79\n",
      "Step  331: loss 2.32714, time 00.79\n",
      "Step  332: loss 2.17217, time 00.78\n",
      "Step  333: loss 2.37783, time 00.79\n",
      "Step  334: loss 2.14781, time 00.78\n",
      "Step  335: loss 2.07231, time 00.78\n",
      "Step  336: loss 2.17231, time 00.78\n",
      "Step  337: loss 2.10616, time 00.80\n",
      "Step  338: loss 2.21985, time 00.82\n",
      "Step  339: loss 2.26996, time 00.82\n",
      "Step  340: loss 2.30154, time 00.79\n",
      "Step  341: loss 2.15702, time 00.78\n",
      "Step  342: loss 2.46060, time 00.78\n",
      "Step  343: loss 2.34584, time 00.78\n",
      "Step  344: loss 2.32882, time 00.78\n",
      "Step  345: loss 2.12742, time 00.78\n",
      "Step  346: loss 2.23260, time 00.78\n",
      "Step  347: loss 2.34370, time 00.78\n",
      "Step  348: loss 2.33302, time 00.78\n",
      "Step  349: loss 2.32107, time 00.78\n",
      "Step  350: loss 2.41413, time 00.78\n",
      "Step  351: loss 2.27372, time 00.78\n",
      "Step  352: loss 2.27601, time 00.78\n",
      "Step  353: loss 2.22674, time 00.78\n",
      "Step  354: loss 2.50903, time 00.78\n",
      "Step  355: loss 2.44611, time 00.78\n",
      "Step  356: loss 2.28652, time 00.78\n",
      "Step  357: loss 2.04812, time 00.78\n",
      "Step  358: loss 2.28775, time 00.78\n",
      "Step  359: loss 2.27316, time 00.78\n",
      "Step  360: loss 2.24764, time 00.78\n",
      "Step  361: loss 2.47051, time 00.78\n",
      "Step  362: loss 2.07700, time 00.78\n",
      "Step  363: loss 2.16895, time 00.78\n",
      "Step  364: loss 2.24590, time 00.78\n",
      "Step  365: loss 2.10514, time 00.78\n",
      "Step  366: loss 2.26975, time 00.78\n",
      "Step  367: loss 2.39481, time 00.78\n",
      "Step  368: loss 2.22146, time 00.78\n",
      "Step  369: loss 2.27687, time 00.78\n",
      "Step  370: loss 2.29679, time 00.78\n",
      "Step  371: loss 2.31499, time 00.78\n",
      "Step  372: loss 2.12653, time 00.78\n",
      "Step  373: loss 2.16455, time 00.78\n",
      "Step  374: loss 2.27299, time 00.78\n",
      "Step  375: loss 2.13019, time 00.78\n",
      "Step  376: loss 2.22337, time 00.79\n",
      "Step  377: loss 2.18732, time 00.79\n",
      "Step  378: loss 2.12234, time 00.80\n",
      "Step  379: loss 2.14873, time 00.79\n",
      "Step  380: loss 2.33311, time 00.80\n",
      "Step  381: loss 2.35956, time 00.79\n",
      "Step  382: loss 2.09689, time 00.81\n",
      "Step  383: loss 2.10935, time 00.79\n",
      "Step  384: loss 2.24250, time 00.83\n",
      "Step  385: loss 2.26477, time 00.80\n",
      "Step  386: loss 2.13570, time 00.80\n",
      "Step  387: loss 2.04985, time 00.80\n",
      "Step  388: loss 2.27008, time 00.81\n",
      "Step  389: loss 2.22214, time 00.79\n",
      "Step  390: loss 2.15537, time 00.80\n",
      "Step  391: loss 2.29800, time 00.79\n",
      "Step  392: loss 2.46689, time 00.79\n",
      "Step  393: loss 2.14278, time 00.79\n",
      "Step  394: loss 2.33713, time 00.80\n",
      "Step  395: loss 2.27192, time 00.79\n",
      "Step  396: loss 2.39347, time 00.81\n",
      "Step  397: loss 2.17662, time 00.80\n",
      "Step  398: loss 2.16714, time 00.80\n",
      "Step  399: loss 2.21605, time 00.79\n",
      "Step  400: loss 2.37925, time 00.81\n",
      "Step  401: loss 2.22759, time 00.81\n",
      "Step  402: loss 2.22953, time 00.81\n",
      "Step  403: loss 2.22910, time 00.79\n",
      "Step  404: loss 2.26337, time 00.78\n",
      "Step  405: loss 2.24372, time 00.78\n",
      "Step  406: loss 2.11886, time 00.78\n",
      "Step  407: loss 2.15097, time 00.78\n",
      "Step  408: loss 2.16071, time 00.79\n",
      "Step  409: loss 2.20271, time 00.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step  410: loss 2.24323, time 00.78\n",
      "Step  411: loss 2.03518, time 00.78\n",
      "Step  412: loss 2.24604, time 00.78\n",
      "Step  413: loss 2.12363, time 00.78\n",
      "Step  414: loss 2.29219, time 00.78\n",
      "Step  415: loss 2.15361, time 00.78\n",
      "Step  416: loss 2.39748, time 00.78\n",
      "Step  417: loss 2.12682, time 00.78\n",
      "Step  418: loss 2.17833, time 00.78\n",
      "Step  419: loss 2.41134, time 00.78\n",
      "Step  420: loss 2.17703, time 00.78\n",
      "Step  421: loss 2.36053, time 00.78\n",
      "Step  422: loss 2.10723, time 00.78\n",
      "Step  423: loss 2.18072, time 00.79\n",
      "Step  424: loss 2.27325, time 00.78\n",
      "Step  425: loss 2.31062, time 00.78\n",
      "Step  426: loss 2.22602, time 00.78\n",
      "Step  427: loss 2.15929, time 00.79\n",
      "Step  428: loss 2.16113, time 00.78\n",
      "Step  429: loss 2.19535, time 00.78\n",
      "Step  430: loss 2.16268, time 00.78\n",
      "Step  431: loss 2.16835, time 00.79\n",
      "Step  432: loss 2.12362, time 00.80\n",
      "Step  433: loss 2.10344, time 00.80\n",
      "Step  434: loss 2.09710, time 00.81\n",
      "Step  435: loss 2.18434, time 00.79\n",
      "Step  436: loss 2.15428, time 00.78\n",
      "Step  437: loss 2.35985, time 00.79\n",
      "Step  438: loss 2.18788, time 00.78\n",
      "Step  439: loss 2.38409, time 00.78\n",
      "Step  440: loss 2.22657, time 00.79\n",
      "Step  441: loss 1.99773, time 00.78\n",
      "Step  442: loss 2.24337, time 00.78\n",
      "Step  443: loss 2.32142, time 00.78\n",
      "Step  444: loss 2.26656, time 00.78\n",
      "Step  445: loss 2.17445, time 00.78\n",
      "Step  446: loss 2.12690, time 00.78\n",
      "Step  447: loss 2.21196, time 00.78\n",
      "Step  448: loss 2.06748, time 00.78\n",
      "Step  449: loss 2.29131, time 00.79\n",
      "Step  450: loss 2.12254, time 00.81\n",
      "Step  451: loss 2.04545, time 00.80\n",
      "Step  452: loss 2.15710, time 00.80\n",
      "Step  453: loss 2.13717, time 00.82\n",
      "Step  454: loss 2.24550, time 00.79\n",
      "Step  455: loss 2.26877, time 00.81\n",
      "Step  456: loss 2.10444, time 00.82\n",
      "Step  457: loss 1.96178, time 00.80\n",
      "Step  458: loss 2.19088, time 00.80\n",
      "Step  459: loss 2.08365, time 00.79\n",
      "Step  460: loss 2.05800, time 00.78\n",
      "Step  461: loss 2.13993, time 00.78\n",
      "Step  462: loss 2.13170, time 00.78\n",
      "Step  463: loss 2.01840, time 00.78\n",
      "Step  464: loss 2.15261, time 00.78\n",
      "Step  465: loss 2.03440, time 00.78\n",
      "Step  466: loss 2.22190, time 00.78\n",
      "Step  467: loss 2.20622, time 00.78\n",
      "Step  468: loss 2.10235, time 00.78\n",
      "Step  469: loss 2.01994, time 00.78\n",
      "Step  470: loss 2.21070, time 00.80\n",
      "Step  471: loss 2.15692, time 00.81\n",
      "Step  472: loss 2.03287, time 00.79\n",
      "Step  473: loss 2.05614, time 00.83\n",
      "Step  474: loss 2.12646, time 00.78\n",
      "Step  475: loss 2.36056, time 00.78\n",
      "Step  476: loss 2.22322, time 00.79\n",
      "Step  477: loss 2.09819, time 00.79\n",
      "Step  478: loss 2.17684, time 00.79\n",
      "Step  479: loss 2.18189, time 00.78\n",
      "Step  480: loss 2.13383, time 00.78\n",
      "Step  481: loss 2.12960, time 00.80\n",
      "Step  482: loss 2.13262, time 00.79\n",
      "Step  483: loss 2.27323, time 00.78\n",
      "Step  484: loss 2.03324, time 00.79\n",
      "Step  485: loss 2.31641, time 00.79\n",
      "Step  486: loss 2.19119, time 00.81\n",
      "Step  487: loss 2.28402, time 00.81\n",
      "Step  488: loss 2.11781, time 00.78\n",
      "Step  489: loss 2.17797, time 00.78\n",
      "Step  490: loss 2.29480, time 00.78\n",
      "Step  491: loss 2.11280, time 00.78\n",
      "Step  492: loss 2.07934, time 00.78\n",
      "Step  493: loss 2.10119, time 00.78\n",
      "Step  494: loss 2.22423, time 00.78\n",
      "Step  495: loss 2.05758, time 00.78\n",
      "Step  496: loss 2.18307, time 00.78\n",
      "Step  497: loss 2.30023, time 00.78\n",
      "Step  498: loss 2.16818, time 00.79\n",
      "Step  499: loss 2.14672, time 00.81\n",
      "Step  500: loss 2.07485, time 00.80\n",
      "Step  501: loss 2.01966, time 00.79\n",
      "Step  502: loss 1.96611, time 00.78\n",
      "Step  503: loss 2.23068, time 00.78\n",
      "Step  504: loss 2.19609, time 00.78\n",
      "Step  505: loss 1.97901, time 00.78\n",
      "Step  506: loss 2.13867, time 00.79\n",
      "Step  507: loss 2.23826, time 00.78\n",
      "Step  508: loss 2.12176, time 00.79\n",
      "Step  509: loss 2.24532, time 00.78\n",
      "Step  510: loss 2.15779, time 00.78\n",
      "Step  511: loss 1.95527, time 00.78\n",
      "Step  512: loss 2.21228, time 00.78\n",
      "Step  513: loss 2.14012, time 00.78\n",
      "Step  514: loss 2.08658, time 00.78\n",
      "Step  515: loss 2.13336, time 00.78\n",
      "Step  516: loss 2.16088, time 00.78\n",
      "Step  517: loss 2.15198, time 00.78\n",
      "Step  518: loss 2.23266, time 00.78\n",
      "Step  519: loss 1.98285, time 00.78\n",
      "Step  520: loss 1.92997, time 00.78\n",
      "Step  521: loss 1.92413, time 00.78\n",
      "Step  522: loss 2.12238, time 00.78\n",
      "Step  523: loss 2.22402, time 00.78\n",
      "Step  524: loss 2.11575, time 00.79\n",
      "Step  525: loss 1.90598, time 00.78\n",
      "Step  526: loss 1.99059, time 00.78\n",
      "Step  527: loss 1.82749, time 00.78\n",
      "Step  528: loss 2.08768, time 00.78\n",
      "Step  529: loss 1.87777, time 00.79\n",
      "Step  530: loss 2.10286, time 00.78\n",
      "Step  531: loss 2.11670, time 00.78\n",
      "Step  532: loss 2.00547, time 00.78\n",
      "Step  533: loss 2.07421, time 00.78\n",
      "Step  534: loss 1.96502, time 00.78\n",
      "Step  535: loss 2.15829, time 00.78\n",
      "Step  536: loss 1.95824, time 00.78\n",
      "Step  537: loss 2.08334, time 00.78\n",
      "Step  538: loss 2.06182, time 00.78\n",
      "Step  539: loss 1.96129, time 00.79\n",
      "Step  540: loss 1.98634, time 00.78\n",
      "Step  541: loss 1.99525, time 00.78\n",
      "Step  542: loss 2.13883, time 00.79\n",
      "Step  543: loss 2.12460, time 00.78\n",
      "Step  544: loss 2.14659, time 00.78\n",
      "Step  545: loss 1.96122, time 00.78\n",
      "Step  546: loss 1.92308, time 00.78\n",
      "Step  547: loss 2.04919, time 00.78\n",
      "Step  548: loss 2.17315, time 00.79\n",
      "Step  549: loss 2.00173, time 00.78\n",
      "Step  550: loss 1.93519, time 00.78\n",
      "Step  551: loss 1.98636, time 00.79\n",
      "Step  552: loss 2.02437, time 00.78\n",
      "Step  553: loss 2.03817, time 00.79\n",
      "Step  554: loss 2.03005, time 00.79\n",
      "Step  555: loss 1.95551, time 00.78\n",
      "Step  556: loss 1.97668, time 00.79\n",
      "Step  557: loss 1.93030, time 00.78\n",
      "Step  558: loss 1.85361, time 00.79\n",
      "Step  559: loss 2.02225, time 00.78\n",
      "Step  560: loss 2.20602, time 00.78\n",
      "Step  561: loss 2.15220, time 00.78\n",
      "Step  562: loss 2.22976, time 00.78\n",
      "Step  563: loss 1.96100, time 00.78\n",
      "Step  564: loss 1.82670, time 00.79\n",
      "Step  565: loss 2.03548, time 00.78\n",
      "Step  566: loss 1.84873, time 00.78\n",
      "Step  567: loss 2.06006, time 00.78\n",
      "Step  568: loss 1.81854, time 00.79\n",
      "Step  569: loss 2.00384, time 00.78\n",
      "Step  570: loss 2.25377, time 00.78\n",
      "Step  571: loss 1.99749, time 00.78\n",
      "Step  572: loss 2.06016, time 00.78\n",
      "Step  573: loss 1.91844, time 00.78\n",
      "Step  574: loss 1.97760, time 00.78\n",
      "Step  575: loss 2.03917, time 00.78\n",
      "Step  576: loss 2.14795, time 00.78\n",
      "Step  577: loss 2.00087, time 00.78\n",
      "Step  578: loss 2.08347, time 00.81\n",
      "Step  579: loss 1.98163, time 00.80\n",
      "Step  580: loss 2.06954, time 00.30\n",
      "Evaluate step  580: loss 2.70300, time 00.67\n",
      "Evaluate step  580: loss 2.71356, time 01.38\n",
      "Evaluate step  580: loss 2.74687, time 02.06\n",
      "Evaluate step  580: loss 2.72911, time 02.74\n",
      "Evaluate step  580: loss 2.68532, time 03.45\n",
      "Evaluate step  580: loss 2.72901, time 04.14\n",
      "Evaluate step  580: loss 2.73920, time 04.83\n",
      "Evaluate step  580: loss 2.67545, time 05.53\n",
      "Evaluate step  580: loss 2.73114, time 06.23\n",
      "Evaluate step  580: loss 2.68468, time 06.92\n",
      "Evaluate step  580: loss 2.73460, time 07.62\n",
      "Evaluate step  580: loss 2.73610, time 08.29\n",
      "Evaluate step  580: loss 2.81902, time 08.97\n",
      "Evaluate step  580: loss 2.71516, time 09.66\n",
      "Evaluate step  580: loss 2.74139, time 10.35\n",
      "Evaluate step  580: loss 2.72561, time 11.04\n",
      "Evaluate step  580: loss 2.71571, time 11.73\n",
      "Evaluate step  580: loss 2.68622, time 12.42\n",
      "Evaluate step  580: loss 2.73588, time 13.11\n",
      "Evaluate step  580: loss 2.71426, time 13.79\n",
      "Evaluate step  580: loss 2.71081, time 14.48\n",
      "Evaluate step  580: loss 2.69852, time 15.16\n",
      "Evaluate step  580: loss 2.73958, time 15.87\n",
      "Evaluate step  580: loss 2.75773, time 16.56\n",
      "Evaluate step  580: loss 2.72698, time 17.24\n",
      "Evaluate step  580: loss 2.69394, time 17.91\n",
      "Evaluate step  580: loss 2.71835, time 18.58\n",
      "Evaluate step  580: loss 2.70949, time 19.27\n",
      "Evaluate step  580: loss 2.70721, time 19.95\n",
      "Evaluate step  580: loss 2.68634, time 20.65\n",
      "Evaluate step  580: loss 2.75611, time 21.33\n",
      "Evaluate step  580: loss 2.69860, time 22.05\n",
      "Evaluate step  580: loss 2.71182, time 22.75\n",
      "Evaluate step  580: loss 2.70223, time 23.43\n",
      "Evaluate step  580: loss 2.71733, time 24.14\n",
      "Evaluate step  580: loss 2.67961, time 24.68\n",
      "Evaluate total loss: 2.718775795565711, avg. time: 0.7356790065765381\n",
      "Step  581: loss 1.86107, time 00.81\n",
      "Step  582: loss 1.95585, time 00.80\n",
      "Step  583: loss 2.04022, time 00.80\n",
      "Step  584: loss 2.00610, time 00.81\n",
      "Step  585: loss 1.90519, time 00.81\n",
      "Step  586: loss 1.89362, time 00.80\n",
      "Step  587: loss 1.87473, time 00.80\n",
      "Step  588: loss 2.11981, time 00.79\n",
      "Step  589: loss 1.95854, time 00.80\n",
      "Step  590: loss 1.97703, time 00.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step  591: loss 2.15176, time 00.81\n",
      "Step  592: loss 2.06471, time 00.83\n",
      "Step  593: loss 2.08108, time 00.81\n",
      "Step  594: loss 2.12249, time 00.80\n",
      "Step  595: loss 2.03235, time 00.81\n",
      "Step  596: loss 1.85455, time 00.81\n",
      "Step  597: loss 1.90647, time 00.82\n",
      "Step  598: loss 1.82505, time 00.80\n",
      "Step  599: loss 1.89458, time 00.81\n",
      "Step  600: loss 2.06514, time 00.80\n",
      "Step  601: loss 2.01493, time 00.78\n",
      "Step  602: loss 1.67609, time 00.78\n",
      "Step  603: loss 2.06158, time 00.78\n",
      "Step  604: loss 1.99244, time 00.78\n",
      "Step  605: loss 1.92865, time 00.78\n",
      "Step  606: loss 1.89609, time 00.78\n",
      "Step  607: loss 1.98338, time 00.78\n",
      "Step  608: loss 1.71981, time 00.78\n",
      "Step  609: loss 1.93727, time 00.78\n",
      "Step  610: loss 1.96130, time 00.78\n",
      "Step  611: loss 1.99006, time 00.79\n",
      "Step  612: loss 1.95134, time 00.78\n",
      "Step  613: loss 2.07106, time 00.79\n",
      "Step  614: loss 2.02046, time 00.78\n",
      "Step  615: loss 2.02679, time 00.79\n",
      "Step  616: loss 1.93850, time 00.78\n",
      "Step  617: loss 1.98558, time 00.78\n",
      "Step  618: loss 2.05011, time 00.79\n",
      "Step  619: loss 1.93166, time 00.78\n",
      "Step  620: loss 1.81892, time 00.79\n",
      "Step  621: loss 1.93104, time 00.78\n",
      "Step  622: loss 1.89761, time 00.78\n",
      "Step  623: loss 1.95389, time 00.78\n",
      "Step  624: loss 2.01450, time 00.78\n",
      "Step  625: loss 1.97139, time 00.79\n",
      "Step  626: loss 1.83504, time 00.78\n",
      "Step  627: loss 1.93265, time 00.78\n",
      "Step  628: loss 1.72484, time 00.78\n",
      "Step  629: loss 2.00978, time 00.78\n",
      "Step  630: loss 1.93379, time 00.78\n",
      "Step  631: loss 2.13881, time 00.79\n",
      "Step  632: loss 2.09969, time 00.78\n",
      "Step  633: loss 1.79454, time 00.78\n",
      "Step  634: loss 1.72694, time 00.78\n",
      "Step  635: loss 1.83607, time 00.78\n",
      "Step  636: loss 1.86447, time 00.78\n",
      "Step  637: loss 1.87204, time 00.78\n",
      "Step  638: loss 1.78256, time 00.78\n",
      "Step  639: loss 1.84367, time 00.78\n",
      "Step  640: loss 2.15647, time 00.78\n",
      "Step  641: loss 1.84415, time 00.78\n",
      "Step  642: loss 1.79482, time 00.79\n",
      "Step  643: loss 1.85079, time 00.79\n",
      "Step  644: loss 1.65585, time 00.79\n",
      "Step  645: loss 1.92044, time 00.79\n",
      "Step  646: loss 2.07561, time 00.79\n",
      "Step  647: loss 1.95498, time 00.78\n",
      "Step  648: loss 1.98117, time 00.78\n",
      "Step  649: loss 1.76304, time 00.78\n",
      "Step  650: loss 2.02944, time 00.78\n",
      "Step  651: loss 1.91485, time 00.78\n",
      "Step  652: loss 2.08592, time 00.78\n",
      "Step  653: loss 1.87303, time 00.78\n",
      "Step  654: loss 1.67647, time 00.78\n",
      "Step  655: loss 1.87673, time 00.81\n",
      "Step  656: loss 1.87979, time 00.80\n",
      "Step  657: loss 1.86171, time 00.80\n",
      "Step  658: loss 1.85862, time 00.79\n",
      "Step  659: loss 2.01416, time 00.79\n",
      "Step  660: loss 1.86281, time 00.78\n",
      "Step  661: loss 1.77484, time 00.78\n",
      "Step  662: loss 1.91059, time 00.79\n",
      "Step  663: loss 1.83906, time 00.80\n",
      "Step  664: loss 1.85958, time 00.79\n",
      "Step  665: loss 1.94735, time 00.78\n",
      "Step  666: loss 1.73026, time 00.78\n",
      "Step  667: loss 1.70401, time 00.78\n",
      "Step  668: loss 1.76296, time 00.79\n",
      "Step  669: loss 1.92515, time 00.78\n",
      "Step  670: loss 1.91397, time 00.78\n",
      "Step  671: loss 2.17657, time 00.78\n",
      "Step  672: loss 1.77453, time 00.79\n",
      "Step  673: loss 1.97795, time 00.80\n",
      "Step  674: loss 1.85304, time 00.81\n",
      "Step  675: loss 1.91276, time 00.80\n",
      "Step  676: loss 1.67994, time 00.79\n",
      "Step  677: loss 1.79972, time 00.79\n",
      "Step  678: loss 1.76354, time 00.78\n",
      "Step  679: loss 1.64331, time 00.78\n",
      "Step  680: loss 1.83233, time 00.78\n",
      "Step  681: loss 1.90136, time 00.79\n",
      "Step  682: loss 2.07235, time 00.78\n",
      "Step  683: loss 1.73914, time 00.78\n",
      "Step  684: loss 1.60198, time 00.78\n",
      "Step  685: loss 1.90446, time 00.78\n",
      "Step  686: loss 2.08302, time 00.78\n",
      "Step  687: loss 1.76410, time 00.79\n",
      "Step  688: loss 1.86589, time 00.78\n",
      "Step  689: loss 1.69746, time 00.79\n",
      "Step  690: loss 1.87304, time 00.78\n",
      "Step  691: loss 1.85785, time 00.78\n",
      "Step  692: loss 1.94702, time 00.78\n",
      "Step  693: loss 1.75998, time 00.78\n",
      "Step  694: loss 1.79904, time 00.79\n",
      "Step  695: loss 1.88089, time 00.79\n",
      "Step  696: loss 1.78343, time 00.78\n",
      "Step  697: loss 1.90300, time 00.78\n",
      "Step  698: loss 1.67895, time 00.78\n",
      "Step  699: loss 1.58521, time 00.79\n",
      "Step  700: loss 1.71996, time 00.78\n",
      "Step  701: loss 1.82476, time 00.79\n",
      "Step  702: loss 1.72944, time 00.79\n",
      "Step  703: loss 1.91102, time 00.79\n",
      "Step  704: loss 1.86259, time 00.82\n",
      "Step  705: loss 1.85413, time 00.80\n",
      "Step  706: loss 1.86546, time 00.81\n",
      "Step  707: loss 1.95028, time 00.79\n",
      "Step  708: loss 1.87349, time 00.78\n",
      "Step  709: loss 1.59248, time 00.81\n",
      "Step  710: loss 1.66578, time 00.80\n",
      "Step  711: loss 1.76679, time 00.79\n",
      "Step  712: loss 1.93230, time 00.80\n",
      "Step  713: loss 1.98324, time 00.80\n",
      "Step  714: loss 1.91137, time 00.80\n",
      "Step  715: loss 1.69650, time 00.80\n",
      "Step  716: loss 1.73505, time 00.79\n",
      "Step  717: loss 1.85792, time 00.81\n",
      "Step  718: loss 1.85496, time 00.82\n",
      "Step  719: loss 1.81393, time 00.80\n",
      "Step  720: loss 1.80349, time 00.80\n",
      "Step  721: loss 1.82799, time 00.79\n",
      "Step  722: loss 1.80273, time 00.79\n",
      "Step  723: loss 1.68854, time 00.82\n",
      "Step  724: loss 2.02049, time 00.80\n",
      "Step  725: loss 1.81992, time 00.78\n",
      "Step  726: loss 1.82236, time 00.78\n",
      "Step  727: loss 1.76524, time 00.78\n",
      "Step  728: loss 1.83883, time 00.78\n",
      "Step  729: loss 1.67779, time 00.78\n",
      "Step  730: loss 1.70776, time 00.78\n",
      "Step  731: loss 1.98180, time 00.78\n",
      "Step  732: loss 1.78530, time 00.78\n",
      "Step  733: loss 1.83724, time 00.79\n",
      "Step  734: loss 1.84962, time 00.79\n",
      "Step  735: loss 1.83074, time 00.79\n",
      "Step  736: loss 1.62347, time 00.78\n",
      "Step  737: loss 1.79900, time 00.79\n",
      "Step  738: loss 1.96320, time 00.78\n",
      "Step  739: loss 1.81900, time 00.78\n",
      "Step  740: loss 1.71329, time 00.78\n",
      "Step  741: loss 1.95001, time 00.78\n",
      "Step  742: loss 1.90387, time 00.79\n",
      "Step  743: loss 1.83686, time 00.78\n",
      "Step  744: loss 1.67086, time 00.78\n",
      "Step  745: loss 1.83173, time 00.78\n",
      "Step  746: loss 1.96125, time 00.78\n",
      "Step  747: loss 1.82915, time 00.78\n",
      "Step  748: loss 1.82506, time 00.80\n",
      "Step  749: loss 1.81710, time 00.80\n",
      "Step  750: loss 1.83167, time 00.80\n",
      "Step  751: loss 1.94252, time 00.82\n",
      "Step  752: loss 1.56345, time 00.78\n",
      "Step  753: loss 1.86353, time 00.78\n",
      "Step  754: loss 1.72294, time 00.78\n",
      "Step  755: loss 1.60946, time 00.78\n",
      "Step  756: loss 1.58026, time 00.78\n",
      "Step  757: loss 1.72907, time 00.78\n",
      "Step  758: loss 1.69077, time 00.79\n",
      "Step  759: loss 1.58327, time 00.78\n",
      "Step  760: loss 1.57785, time 00.78\n",
      "Step  761: loss 1.72497, time 00.79\n",
      "Step  762: loss 1.72352, time 00.79\n",
      "Step  763: loss 1.77988, time 00.79\n",
      "Step  764: loss 2.21918, time 00.79\n",
      "Step  765: loss 1.68082, time 00.78\n",
      "Step  766: loss 1.85789, time 00.79\n",
      "Step  767: loss 1.72448, time 00.78\n",
      "Step  768: loss 1.77452, time 00.78\n",
      "Step  769: loss 1.48422, time 00.78\n",
      "Step  770: loss 1.81314, time 00.79\n",
      "Step  771: loss 1.64483, time 00.79\n",
      "Step  772: loss 1.62641, time 00.78\n",
      "Step  773: loss 1.64237, time 00.79\n",
      "Step  774: loss 1.63472, time 00.79\n",
      "Step  775: loss 1.65507, time 00.79\n",
      "Step  776: loss 1.56181, time 00.79\n",
      "Step  777: loss 1.95575, time 00.79\n",
      "Step  778: loss 1.71665, time 00.79\n",
      "Step  779: loss 1.88011, time 00.78\n",
      "Step  780: loss 1.78253, time 00.78\n",
      "Step  781: loss 1.73006, time 00.79\n",
      "Step  782: loss 1.89520, time 00.79\n",
      "Step  783: loss 1.86820, time 00.78\n",
      "Step  784: loss 1.79407, time 00.78\n",
      "Step  785: loss 1.68262, time 00.79\n",
      "Step  786: loss 1.85314, time 00.79\n",
      "Step  787: loss 1.68604, time 00.79\n",
      "Step  788: loss 1.58200, time 00.78\n",
      "Step  789: loss 1.77570, time 00.78\n",
      "Step  790: loss 1.68097, time 00.79\n",
      "Step  791: loss 1.75594, time 00.79\n",
      "Step  792: loss 1.90212, time 00.79\n",
      "Step  793: loss 1.77532, time 00.83\n",
      "Step  794: loss 1.78144, time 00.81\n",
      "Step  795: loss 1.61068, time 00.79\n",
      "Step  796: loss 1.59718, time 00.80\n",
      "Step  797: loss 1.94930, time 00.80\n",
      "Step  798: loss 1.72967, time 00.80\n",
      "Step  799: loss 1.78353, time 00.81\n",
      "Step  800: loss 1.70888, time 00.81\n",
      "Step  801: loss 1.69926, time 00.80\n",
      "Step  802: loss 1.78144, time 00.79\n",
      "Step  803: loss 1.64975, time 00.82\n",
      "Step  804: loss 1.86974, time 00.80\n",
      "Step  805: loss 1.78027, time 00.81\n",
      "Step  806: loss 1.78000, time 00.82\n",
      "Step  807: loss 1.74232, time 00.79\n",
      "Step  808: loss 1.88810, time 00.80\n",
      "Step  809: loss 1.75613, time 00.80\n",
      "Step  810: loss 1.81059, time 00.80\n",
      "Step  811: loss 1.62085, time 00.79\n",
      "Step  812: loss 1.56143, time 00.81\n",
      "Step  813: loss 1.55400, time 00.81\n",
      "Step  814: loss 1.70880, time 00.79\n",
      "Step  815: loss 1.65719, time 00.80\n",
      "Step  816: loss 1.66778, time 00.80\n",
      "Step  817: loss 1.64759, time 00.80\n",
      "Step  818: loss 1.84478, time 00.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step  819: loss 1.52862, time 00.80\n",
      "Step  820: loss 1.63110, time 00.80\n",
      "Step  821: loss 1.62371, time 00.82\n",
      "Step  822: loss 1.57116, time 00.78\n",
      "Step  823: loss 1.74438, time 00.79\n",
      "Step  824: loss 1.60686, time 00.78\n",
      "Step  825: loss 1.67128, time 00.78\n",
      "Step  826: loss 1.51286, time 00.78\n",
      "Step  827: loss 1.61175, time 00.78\n",
      "Step  828: loss 1.86049, time 00.79\n",
      "Step  829: loss 1.56298, time 00.79\n",
      "Step  830: loss 1.66946, time 00.78\n",
      "Step  831: loss 1.74453, time 00.79\n",
      "Step  832: loss 1.76421, time 00.78\n",
      "Step  833: loss 1.79965, time 00.79\n",
      "Step  834: loss 1.58927, time 00.78\n",
      "Step  835: loss 1.59168, time 00.78\n",
      "Step  836: loss 1.85803, time 00.78\n",
      "Step  837: loss 1.65099, time 00.78\n",
      "Step  838: loss 1.67474, time 00.78\n",
      "Step  839: loss 1.49890, time 00.78\n",
      "Step  840: loss 1.66144, time 00.78\n",
      "Step  841: loss 1.67451, time 00.79\n",
      "Step  842: loss 1.71483, time 00.79\n",
      "Step  843: loss 1.52363, time 00.78\n",
      "Step  844: loss 1.86510, time 00.78\n",
      "Step  845: loss 1.59668, time 00.78\n",
      "Step  846: loss 1.67975, time 00.78\n",
      "Step  847: loss 1.90995, time 00.78\n",
      "Step  848: loss 1.77014, time 00.78\n",
      "Step  849: loss 1.68789, time 00.78\n",
      "Step  850: loss 1.64789, time 00.79\n",
      "Step  851: loss 1.92101, time 00.78\n",
      "Step  852: loss 1.50191, time 00.78\n",
      "Step  853: loss 1.72922, time 00.78\n",
      "Step  854: loss 1.50134, time 00.78\n",
      "Step  855: loss 1.45616, time 00.79\n",
      "Step  856: loss 1.79393, time 00.78\n",
      "Step  857: loss 1.79460, time 00.78\n",
      "Step  858: loss 1.44543, time 00.79\n",
      "Step  859: loss 1.56829, time 00.82\n",
      "Step  860: loss 1.59414, time 00.81\n",
      "Step  861: loss 1.79366, time 00.79\n",
      "Step  862: loss 2.00767, time 00.78\n",
      "Step  863: loss 1.70182, time 00.79\n",
      "Step  864: loss 1.60694, time 00.78\n",
      "Step  865: loss 1.71556, time 00.78\n",
      "Step  866: loss 1.49345, time 00.78\n",
      "Step  867: loss 1.86605, time 00.79\n",
      "Step  868: loss 1.61050, time 00.79\n",
      "Step  869: loss 1.57322, time 00.81\n",
      "Step  870: loss 1.54444, time 00.31\n",
      "Evaluate step  870: loss 2.42804, time 00.66\n",
      "Evaluate step  870: loss 2.50917, time 01.33\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    train_gen = generator(train_data, 100, len(class2idx), 0.0, True, 400)\n",
    "    val_gen = generator(valid_data, 100, len(class2idx), 0.0, False, 400)\n",
    "    my_trainer.classifier_train_dev_loop(train_gen, val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generator(test_data, 30, len(class2idx), 0.0, False, 400)\n",
    "val_gen = generator(valid_data, 30, len(class2idx), 0.0, False, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer.classifier_evaluate_step(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer.classifier_evaluate_step(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, max_len=400):\n",
    "    text = text[:min(max_len, len(text))]\n",
    "    text = re.sub('\\d+','N',re.sub('\\W+','',re.sub('[ ]+','_',re.sub('[\\n\\r]+','_nl_', text.lower()))))\n",
    "    print(text)\n",
    "    ids = np.array([word2idx[c] for c in text.replace('_nl_', '\\n') if c in word2idx], dtype=np.int32)\n",
    "    if len(ids) < 20:\n",
    "        t = np.zeros(shape=(20), dtype=np.int32)\n",
    "        t[:len(ids)] = ids\n",
    "        ids = t\n",
    "    seq_lens = len(ids)\n",
    "    classifiers = my_trainer.test_classifiers\n",
    "    inputs = np.transpose(np.expand_dims(ids, axis=0), [1, 0])\n",
    "    fd = {\n",
    "        my_trainer.model_test.inputs: inputs,\n",
    "        my_trainer.model_test.seq_lens: np.array([seq_lens], dtype=np.int32),\n",
    "        my_trainer.model_test.reset_state: True\n",
    "    }\n",
    "    pred_ys = my_trainer.session.run([c.out_prob for c in classifiers], feed_dict=fd)\n",
    "    return {\n",
    "        k: v[0][1] for k, v in zip(class2idx, pred_ys)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {k: v for v, k in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [''.join(idx2word[x] for x in y) for y, _ in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(a):\n",
    "    print(d)\n",
    "    print(predict(d))\n",
    "    print(test_data[i][1])\n",
    "    print('-' * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('cch_gim_cn_khoa_hc_nht__gim_cn_l_tp_th_dc_v_n_king_hp_l_tuy_nhin_nu_khng_c_ch__n_ung_hp_l_s_khin_mi_cng_sc_tp_luyn_tr_nn_v_ch_hoc_chng_ta_s_khng__nng_lng__c_th_phc_hi_sau_tp_luyn_vy_chng_ta_n_g_sau_khi_tp_th_dc__gim_cn_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('tng_mu_pastel_nh_nhng_khng_ch_to_nn_khng_kh_vui_ti_ca_ma_xun_m_cn_l_nim_cm_hng_bt_tn_cho_nhng_tm_hnh_sng_o_ca_cng_ng_mng_cng_im_qua_nhng_a_im_th_v_cho_cc_fan_ca_pastel_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
