{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model import LanguageModel\n",
    "import json\n",
    "import numpy as np\n",
    "from utils import batchify, get_batch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'rnn_layers': [{'units': 1150, 'input_size': 400, 'drop_i': 0.65, 'drop_w': 0.5, 'drop_o': 0.3}, {'units': 1150, 'input_size': 1150, 'drop_w': 0.5, 'drop_o': 0.3}, {'units': 400, 'input_size': 1150, 'drop_o': 0.4, 'drop_w': 0.5}], 'vocab_size': 380, 'drop_e': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(**params, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_model()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_restore(session, save_file):\n",
    "    reader = tf.train.NewCheckpointReader(save_file)\n",
    "    saved_shapes = reader.get_variable_to_shape_map()\n",
    "    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\n",
    "        if var.name.split(':')[0] in saved_shapes])\n",
    "    restore_vars = []\n",
    "    name2var = dict(zip(map(lambda x:x.name.split(':')[0], tf.global_variables()), tf.global_variables()))\n",
    "    with tf.variable_scope('', reuse=True):\n",
    "        for var_name, saved_var_name in var_names:\n",
    "            curr_var = name2var[saved_var_name]\n",
    "            var_shape = curr_var.get_shape().as_list()\n",
    "            if var_shape == saved_shapes[saved_var_name]:\n",
    "                restore_vars.append(curr_var)\n",
    "    saver = tf.train.Saver(restore_vars)\n",
    "    saver.restore(session, save_file)\n",
    "    return restore_vars, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from 2/checkpoints/test-53210\n"
     ]
    }
   ],
   "source": [
    "rv, saver = optimistic_restore(sess, '2/checkpoints/test-53210')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'LanguageModel/decoder_b:0' shape=(380,) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/embedding_weight:0' shape=(380, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/lstm_fused_cell/U:0' shape=(1150, 4600) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/lstm_fused_cell/W:0' shape=(400, 4600) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/lstm_fused_cell/bias:0' shape=(4600,) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/lstm_fused_cell_1/U:0' shape=(1150, 4600) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/lstm_fused_cell_1/W:0' shape=(1150, 4600) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/lstm_fused_cell_1/bias:0' shape=(4600,) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/lstm_fused_cell_2/U:0' shape=(400, 1600) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/lstm_fused_cell_2/W:0' shape=(1150, 1600) dtype=float32_ref>,\n",
       " <tf.Variable 'LanguageModel/lstm_fused_cell_2/bias:0' shape=(1600,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2idx.json', 'r') as inp:\n",
    "    word2idx = json.load(inp)\n",
    "idx2word = {k: v for v, k in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(a, temperature=0.5):\n",
    "    a = a / temperature\n",
    "    dist = np.exp(a) / np.sum(np.exp(a))\n",
    "    choices = range(len(a))\n",
    "    return np.random.choice(choices, p=dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(initial_word, gen_len):\n",
    "    init_word = [x for x in initial_word.replace(' ', '_')]\n",
    "    seq_lens = [len(init_word)]\n",
    "    inputs = np.expand_dims(np.array([word2idx[x] for x in init_word]), -1)\n",
    "    # Run the first time\n",
    "    output = sess.run(model.decoder, feed_dict={model.inputs: inputs, model.reset_state: True, model.seq_lens: seq_lens})\n",
    "    next_idx = sample(output[-1, 0, :])\n",
    "    result = [\n",
    "        idx2word[next_idx]\n",
    "    ]\n",
    "    for i in range(1, gen_len):\n",
    "        output = sess.run(model.decoder, feed_dict={model.inputs: [[next_idx]], model.reset_state: False, model.seq_lens: [1]})\n",
    "        next_idx = sample(output[-1, 0, :])\n",
    "        result.append(idx2word[next_idx])\n",
    "    return initial_word + ''.join(x if x != '_' else ' ' for x in result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baomoi/test.npy','rb') as inp:\n",
    "    test = np.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = batchify(test, 530).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(dtype=tf.int32, shape=[None, None], name='y')\n",
    "test_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=model.decoder,\n",
    "    targets=y,\n",
    "    weights=model.seq_masks,\n",
    "    average_across_timesteps=True,\n",
    "    average_across_batch=True,\n",
    "    name='test_loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_step(test_data, bptt):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    step = None\n",
    "    t = 0\n",
    "    for i in range(0, len(test_data), bptt):\n",
    "        next_x, next_y = get_batch(test_data, bptt, i, evaluate=True)\n",
    "        loss = sess.run(\n",
    "            test_loss,\n",
    "            feed_dict={\n",
    "                model.inputs: next_x,\n",
    "                y: next_y,\n",
    "                model.seq_lens: [next_x.shape[0]]*next_x.shape[1],\n",
    "                model.reset_state: i == 0\n",
    "            }\n",
    "        )\n",
    "        total_loss += loss * len(next_x)\n",
    "        print(\"Evaluate loss {}, time {}\".format(\n",
    "            loss, time.time()-start_time))\n",
    "        t += 1\n",
    "    total_time = time.time()-start_time\n",
    "    total_loss /= len(test_data)\n",
    "    print(\"Evaluate total loss {}, time {}, avg.time\".format(\n",
    "        total_loss, total_time, total_time / t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate loss 1.2794289588928223, time 0.6178994178771973\n",
      "Evaluate loss 1.2413350343704224, time 1.2045297622680664\n",
      "Evaluate loss 1.2227312326431274, time 1.7912700176239014\n",
      "Evaluate loss 1.2187833786010742, time 2.3776304721832275\n",
      "Evaluate loss 1.2169562578201294, time 2.963874578475952\n",
      "Evaluate loss 1.2116812467575073, time 3.5499892234802246\n",
      "Evaluate loss 1.225031852722168, time 4.136122226715088\n",
      "Evaluate loss 1.2177261114120483, time 4.72249960899353\n",
      "Evaluate loss 1.2154732942581177, time 5.3103649616241455\n",
      "Evaluate loss 1.2171730995178223, time 5.896874666213989\n",
      "Evaluate loss 1.2257611751556396, time 6.483163595199585\n",
      "Evaluate loss 1.2218865156173706, time 7.069720983505249\n",
      "Evaluate loss 1.231946349143982, time 7.656374454498291\n",
      "Evaluate loss 1.2176791429519653, time 8.243592739105225\n",
      "Evaluate loss 1.2294617891311646, time 8.83069109916687\n",
      "Evaluate loss 1.2303547859191895, time 9.417949676513672\n",
      "Evaluate loss 1.2253262996673584, time 10.005308866500854\n",
      "Evaluate loss 1.2070400714874268, time 10.593258619308472\n",
      "Evaluate loss 1.2168889045715332, time 11.181608438491821\n",
      "Evaluate loss 1.218913197517395, time 11.770682573318481\n",
      "Evaluate loss 1.2127987146377563, time 12.35843276977539\n",
      "Evaluate loss 1.2202951908111572, time 12.945845127105713\n",
      "Evaluate loss 1.2221261262893677, time 13.533162593841553\n",
      "Evaluate loss 1.2001925706863403, time 14.120566129684448\n",
      "Evaluate loss 1.202036738395691, time 14.70783257484436\n",
      "Evaluate loss 1.2121288776397705, time 15.294945240020752\n",
      "Evaluate loss 1.225111961364746, time 15.88309121131897\n",
      "Evaluate loss 1.198862075805664, time 16.470726490020752\n",
      "Evaluate loss 1.206349492073059, time 17.05825662612915\n",
      "Evaluate loss 1.2138317823410034, time 17.64615297317505\n",
      "Evaluate loss 1.207594633102417, time 18.235536336898804\n",
      "Evaluate loss 1.2156517505645752, time 18.822826623916626\n",
      "Evaluate loss 1.1971232891082764, time 19.41074514389038\n",
      "Evaluate loss 1.2053382396697998, time 20.001094341278076\n",
      "Evaluate loss 1.2109575271606445, time 20.592630624771118\n",
      "Evaluate loss 1.2198209762573242, time 21.1846764087677\n",
      "Evaluate loss 1.216647744178772, time 21.776257514953613\n",
      "Evaluate loss 1.2170170545578003, time 22.368184566497803\n",
      "Evaluate loss 1.2251811027526855, time 22.960241317749023\n",
      "Evaluate loss 1.2384780645370483, time 23.551741123199463\n",
      "Evaluate loss 1.2344043254852295, time 24.143903493881226\n",
      "Evaluate loss 1.2207767963409424, time 24.735779523849487\n",
      "Evaluate loss 1.2176544666290283, time 25.32790446281433\n",
      "Evaluate loss 1.2112383842468262, time 25.919679403305054\n",
      "Evaluate loss 1.2271621227264404, time 26.51311182975769\n",
      "Evaluate loss 1.214686632156372, time 27.105616807937622\n",
      "Evaluate loss 1.2224400043487549, time 27.697585582733154\n",
      "Evaluate loss 1.2179018259048462, time 28.289267539978027\n",
      "Evaluate loss 1.212205171585083, time 28.881089687347412\n",
      "Evaluate loss 1.2063993215560913, time 29.474249124526978\n",
      "Evaluate loss 1.2058345079421997, time 30.065890789031982\n",
      "Evaluate loss 1.2289528846740723, time 30.659467220306396\n",
      "Evaluate loss 1.2176876068115234, time 31.251298666000366\n",
      "Evaluate loss 1.2320001125335693, time 31.84327268600464\n",
      "Evaluate loss 1.2091337442398071, time 32.435287952423096\n",
      "Evaluate loss 1.220432162284851, time 33.0276837348938\n",
      "Evaluate loss 1.2263927459716797, time 33.62123131752014\n",
      "Evaluate loss 1.229797124862671, time 34.213335037231445\n",
      "Evaluate loss 1.217602014541626, time 34.8052282333374\n",
      "Evaluate loss 1.2402021884918213, time 35.397324323654175\n",
      "Evaluate loss 1.23294997215271, time 35.98916029930115\n",
      "Evaluate loss 1.2246662378311157, time 36.581048011779785\n",
      "Evaluate loss 1.220582127571106, time 37.1735680103302\n",
      "Evaluate loss 1.237837553024292, time 37.766111612319946\n",
      "Evaluate loss 1.2249101400375366, time 38.35827684402466\n",
      "Evaluate loss 1.2333946228027344, time 38.9507532119751\n",
      "Evaluate loss 1.2346636056900024, time 39.54238319396973\n",
      "Evaluate loss 1.232210397720337, time 40.134279012680054\n",
      "Evaluate loss 1.235388994216919, time 40.72599530220032\n",
      "Evaluate loss 1.2310949563980103, time 41.31768774986267\n",
      "Evaluate loss 1.249415636062622, time 41.909380197525024\n",
      "Evaluate loss 1.2327544689178467, time 42.50112867355347\n",
      "Evaluate loss 1.2390952110290527, time 43.0934898853302\n",
      "Evaluate loss 1.2138537168502808, time 43.68618416786194\n",
      "Evaluate loss 1.2427666187286377, time 44.2786979675293\n",
      "Evaluate loss 1.2328625917434692, time 44.870664834976196\n",
      "Evaluate loss 1.2122321128845215, time 45.46276783943176\n",
      "Evaluate loss 1.2231847047805786, time 46.05587911605835\n",
      "Evaluate loss 1.223106026649475, time 46.65081429481506\n",
      "Evaluate loss 1.2148997783660889, time 47.24540662765503\n",
      "Evaluate loss 1.2320111989974976, time 47.840391397476196\n",
      "Evaluate loss 1.2229807376861572, time 48.436209201812744\n",
      "Evaluate loss 1.210689902305603, time 49.03106355667114\n",
      "Evaluate loss 1.224481463432312, time 49.62629318237305\n",
      "Evaluate loss 1.228253722190857, time 50.22242283821106\n",
      "Evaluate loss 1.2238746881484985, time 50.81751871109009\n",
      "Evaluate loss 1.2329579591751099, time 51.412410259246826\n",
      "Evaluate loss 1.2250372171401978, time 52.00787901878357\n",
      "Evaluate loss 1.2273621559143066, time 52.60266065597534\n",
      "Evaluate loss 1.231580376625061, time 53.19857954978943\n",
      "Evaluate loss 1.2295764684677124, time 53.79319381713867\n",
      "Evaluate loss 1.2213143110275269, time 54.38830327987671\n",
      "Evaluate loss 1.2321829795837402, time 54.984182357788086\n",
      "Evaluate loss 1.2148756980895996, time 55.579203367233276\n",
      "Evaluate loss 1.2126376628875732, time 56.17436861991882\n",
      "Evaluate loss 1.2166990041732788, time 56.77040481567383\n",
      "Evaluate loss 1.2357949018478394, time 57.36526846885681\n",
      "Evaluate loss 1.2124890089035034, time 57.96132040023804\n",
      "Evaluate loss 1.224428653717041, time 58.55719470977783\n",
      "Evaluate loss 1.2271878719329834, time 59.15220069885254\n",
      "Evaluate loss 1.2201780080795288, time 59.74814748764038\n",
      "Evaluate loss 1.2204935550689697, time 60.34354019165039\n",
      "Evaluate loss 1.2235491275787354, time 60.94010591506958\n",
      "Evaluate loss 1.2261693477630615, time 61.53494668006897\n",
      "Evaluate loss 1.2082279920578003, time 62.13000535964966\n",
      "Evaluate loss 1.2259827852249146, time 62.72507882118225\n",
      "Evaluate loss 1.2165032625198364, time 63.32072997093201\n",
      "Evaluate loss 1.2104008197784424, time 63.9157657623291\n",
      "Evaluate loss 1.2095028162002563, time 64.51083898544312\n",
      "Evaluate loss 1.205113172531128, time 65.10583066940308\n",
      "Evaluate loss 1.2139090299606323, time 65.70060849189758\n",
      "Evaluate loss 1.2085928916931152, time 66.29565572738647\n",
      "Evaluate loss 1.206853985786438, time 66.89160561561584\n",
      "Evaluate loss 1.217972755432129, time 67.48716330528259\n",
      "Evaluate loss 1.2233761548995972, time 68.08237719535828\n",
      "Evaluate loss 1.2246530055999756, time 68.67744088172913\n",
      "Evaluate loss 1.2166579961776733, time 69.27266502380371\n",
      "Evaluate loss 1.2149908542633057, time 69.8687002658844\n",
      "Evaluate loss 1.1971662044525146, time 70.46361827850342\n",
      "Evaluate loss 1.210465669631958, time 71.05902218818665\n",
      "Evaluate loss 1.2156904935836792, time 71.6550133228302\n",
      "Evaluate loss 1.1953593492507935, time 72.25086402893066\n",
      "Evaluate loss 1.211757779121399, time 72.84580898284912\n",
      "Evaluate loss 1.2183778285980225, time 73.44084429740906\n",
      "Evaluate loss 1.2267955541610718, time 74.0358636379242\n",
      "Evaluate loss 1.2380905151367188, time 74.63104677200317\n",
      "Evaluate loss 1.2371652126312256, time 75.22676920890808\n",
      "Evaluate loss 1.227132797241211, time 75.82235550880432\n",
      "Evaluate loss 1.2239773273468018, time 76.41756510734558\n",
      "Evaluate loss 1.2224717140197754, time 77.01323866844177\n",
      "Evaluate loss 1.2276407480239868, time 77.60889911651611\n",
      "Evaluate loss 1.22794508934021, time 78.20474910736084\n",
      "Evaluate loss 1.208167552947998, time 78.80049800872803\n",
      "Evaluate loss 1.2161887884140015, time 79.39580869674683\n",
      "Evaluate loss 1.2304704189300537, time 79.99145007133484\n",
      "Evaluate loss 1.2187944650650024, time 80.58761072158813\n",
      "Evaluate loss 1.2220216989517212, time 81.18313789367676\n",
      "Evaluate loss 1.2217590808868408, time 81.77871870994568\n",
      "Evaluate loss 1.2299610376358032, time 82.3742151260376\n",
      "Evaluate loss 1.2248376607894897, time 82.96946907043457\n",
      "Evaluate loss 1.2335269451141357, time 83.56474566459656\n",
      "Evaluate loss 1.2259877920150757, time 84.15984892845154\n",
      "Evaluate loss 1.2141664028167725, time 84.75488710403442\n",
      "Evaluate loss 1.226175308227539, time 85.35001993179321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate loss 1.2245556116104126, time 85.94512629508972\n",
      "Evaluate loss 1.2208575010299683, time 86.54025292396545\n",
      "Evaluate loss 1.2171481847763062, time 87.13532209396362\n",
      "Evaluate loss 1.2137984037399292, time 87.73044419288635\n",
      "Evaluate loss 1.225003957748413, time 88.32616591453552\n",
      "Evaluate loss 1.2169160842895508, time 88.9215931892395\n",
      "Evaluate loss 1.2212707996368408, time 89.51661109924316\n",
      "Evaluate loss 1.2096461057662964, time 90.11299681663513\n",
      "Evaluate loss 1.221401572227478, time 90.7087779045105\n",
      "Evaluate loss 1.217787742614746, time 91.30518817901611\n",
      "Evaluate loss 1.2176214456558228, time 91.90243196487427\n",
      "Evaluate loss 1.2240887880325317, time 92.49833583831787\n",
      "Evaluate loss 1.2347034215927124, time 93.09418559074402\n",
      "Evaluate loss 1.219739556312561, time 93.68912100791931\n",
      "Evaluate loss 1.2149226665496826, time 94.28417658805847\n",
      "Evaluate loss 1.219927430152893, time 94.87947225570679\n",
      "Evaluate loss 1.2366633415222168, time 95.47469139099121\n",
      "Evaluate loss 1.2189414501190186, time 96.06992959976196\n",
      "Evaluate loss 1.2276017665863037, time 96.66526889801025\n",
      "Evaluate loss 1.2170228958129883, time 97.2613480091095\n",
      "Evaluate loss 1.226627230644226, time 97.85683155059814\n",
      "Evaluate loss 1.212971568107605, time 98.45322561264038\n",
      "Evaluate loss 1.218632459640503, time 99.05004644393921\n",
      "Evaluate loss 1.2187836170196533, time 99.64660096168518\n",
      "Evaluate loss 1.2087913751602173, time 100.24241971969604\n",
      "Evaluate loss 1.2117170095443726, time 100.8378164768219\n",
      "Evaluate loss 1.213551640510559, time 101.43469977378845\n",
      "Evaluate loss 1.2141456604003906, time 102.0305552482605\n",
      "Evaluate loss 1.221469521522522, time 102.62707996368408\n",
      "Evaluate loss 1.2136279344558716, time 103.22310590744019\n",
      "Evaluate loss 1.2102266550064087, time 103.81855130195618\n",
      "Evaluate loss 1.2162660360336304, time 104.41432476043701\n",
      "Evaluate loss 1.2238996028900146, time 105.00956511497498\n",
      "Evaluate loss 1.2352495193481445, time 105.60518646240234\n",
      "Evaluate loss 1.2258212566375732, time 106.200528383255\n",
      "Evaluate loss 1.2035573720932007, time 106.79633021354675\n",
      "Evaluate loss 1.2050760984420776, time 107.39189982414246\n",
      "Evaluate loss 1.2232780456542969, time 107.98762702941895\n",
      "Evaluate loss 1.2208384275436401, time 108.58320116996765\n",
      "Evaluate loss 1.2076964378356934, time 109.17878723144531\n",
      "Evaluate loss 1.2142983675003052, time 109.77452230453491\n",
      "Evaluate loss 1.197883129119873, time 110.37113213539124\n",
      "Evaluate loss 1.2121189832687378, time 110.96670627593994\n",
      "Evaluate loss 1.2119637727737427, time 111.56196451187134\n",
      "Evaluate loss 1.2131454944610596, time 112.15754199028015\n",
      "Evaluate loss 1.2145023345947266, time 112.7538685798645\n",
      "Evaluate loss 1.223219394683838, time 113.34977316856384\n",
      "Evaluate loss 1.2313768863677979, time 113.94527792930603\n",
      "Evaluate loss 1.1995164155960083, time 114.54088521003723\n",
      "Evaluate loss 1.2050861120224, time 115.13703298568726\n",
      "Evaluate loss 1.198596715927124, time 115.73233151435852\n",
      "Evaluate loss 1.2166578769683838, time 116.32797646522522\n",
      "Evaluate loss 1.2298829555511475, time 116.92382216453552\n",
      "Evaluate loss 1.2074187994003296, time 117.51906704902649\n",
      "Evaluate loss 1.206060528755188, time 118.11474013328552\n",
      "Evaluate loss 1.21007239818573, time 118.71073698997498\n",
      "Evaluate loss 1.207555890083313, time 119.30577993392944\n",
      "Evaluate loss 1.2461434602737427, time 119.90130066871643\n",
      "Evaluate loss 1.2197552919387817, time 120.49758100509644\n",
      "Evaluate loss 1.221910834312439, time 121.09376573562622\n",
      "Evaluate loss 1.2300877571105957, time 121.68883657455444\n",
      "Evaluate loss 1.2120001316070557, time 122.28465509414673\n",
      "Evaluate loss 1.2218581438064575, time 122.88063740730286\n",
      "Evaluate loss 1.2446951866149902, time 123.47603011131287\n",
      "Evaluate loss 1.2241060733795166, time 124.07213926315308\n",
      "Evaluate loss 1.2294793128967285, time 124.66781568527222\n",
      "Evaluate loss 1.2170547246932983, time 125.26349425315857\n",
      "Evaluate loss 1.2158335447311401, time 125.85880088806152\n",
      "Evaluate loss 1.227888822555542, time 126.45411992073059\n",
      "Evaluate loss 1.241693139076233, time 127.04955387115479\n",
      "Evaluate loss 1.2364261150360107, time 127.64487147331238\n",
      "Evaluate loss 1.2341620922088623, time 128.2404592037201\n",
      "Evaluate loss 1.2297121286392212, time 128.83631658554077\n",
      "Evaluate loss 1.2446211576461792, time 129.43166947364807\n",
      "Evaluate loss 1.2330442667007446, time 130.0269432067871\n",
      "Evaluate loss 1.2186996936798096, time 130.62272095680237\n",
      "Evaluate loss 1.2194836139678955, time 131.21831035614014\n",
      "Evaluate loss 1.2327646017074585, time 131.81401872634888\n",
      "Evaluate loss 1.2285387516021729, time 132.4101424217224\n",
      "Evaluate loss 1.2366567850112915, time 133.00629830360413\n",
      "Evaluate loss 1.2389194965362549, time 133.60173106193542\n",
      "Evaluate loss 1.2358167171478271, time 134.19707703590393\n",
      "Evaluate loss 1.2322124242782593, time 134.792804479599\n",
      "Evaluate loss 1.2260856628417969, time 135.3884060382843\n",
      "Evaluate loss 1.2297842502593994, time 135.98431968688965\n",
      "Evaluate loss 1.211737036705017, time 136.58010149002075\n",
      "Evaluate loss 1.251331090927124, time 137.17578291893005\n",
      "Evaluate loss 1.2330663204193115, time 137.77127766609192\n",
      "Evaluate loss 1.2319303750991821, time 138.3672058582306\n",
      "Evaluate loss 1.2348138093948364, time 138.96265125274658\n",
      "Evaluate loss 1.2186248302459717, time 139.55801153182983\n",
      "Evaluate loss 1.2282145023345947, time 140.1536602973938\n",
      "Evaluate loss 1.2313283681869507, time 140.74908566474915\n",
      "Evaluate loss 1.2174618244171143, time 141.3451750278473\n",
      "Evaluate loss 1.2490133047103882, time 141.94101405143738\n",
      "Evaluate loss 1.227281928062439, time 142.53627252578735\n",
      "Evaluate loss 1.231693148612976, time 143.13225746154785\n",
      "Evaluate loss 1.228093147277832, time 143.72847032546997\n",
      "Evaluate loss 1.2167363166809082, time 144.3241949081421\n",
      "Evaluate loss 1.2182599306106567, time 144.9203326702118\n",
      "Evaluate loss 1.2221686840057373, time 145.5159456729889\n",
      "Evaluate loss 1.2181885242462158, time 146.11198210716248\n",
      "Evaluate loss 1.213402271270752, time 146.70725274085999\n",
      "Evaluate loss 1.217193365097046, time 147.30235171318054\n",
      "Evaluate loss 1.225239872932434, time 147.89779615402222\n",
      "Evaluate loss 1.2224088907241821, time 148.49459958076477\n",
      "Evaluate loss 1.2352244853973389, time 149.09008359909058\n",
      "Evaluate loss 1.2332069873809814, time 149.6853632926941\n",
      "Evaluate loss 1.2434608936309814, time 150.28298926353455\n",
      "Evaluate loss 1.2308666706085205, time 150.8793876171112\n",
      "Evaluate loss 1.214835524559021, time 151.47511458396912\n",
      "Evaluate loss 1.2328473329544067, time 152.07055854797363\n",
      "Evaluate loss 1.2346484661102295, time 152.6661660671234\n",
      "Evaluate loss 1.232010006904602, time 153.2616593837738\n",
      "Evaluate loss 1.2102868556976318, time 153.85867595672607\n",
      "Evaluate loss 1.2253100872039795, time 154.4541745185852\n",
      "Evaluate loss 1.2428349256515503, time 155.0519950389862\n",
      "Evaluate loss 1.231980323791504, time 155.64737224578857\n",
      "Evaluate loss 1.2262241840362549, time 156.243177652359\n",
      "Evaluate loss 1.2427936792373657, time 156.83869910240173\n",
      "Evaluate loss 1.2289115190505981, time 157.43472003936768\n",
      "Evaluate loss 1.235361099243164, time 158.03032898902893\n",
      "Evaluate loss 1.2233209609985352, time 158.6260006427765\n",
      "Evaluate loss 1.2202253341674805, time 159.2211618423462\n",
      "Evaluate loss 1.222814917564392, time 159.81742119789124\n",
      "Evaluate loss 1.2321518659591675, time 160.41351890563965\n",
      "Evaluate loss 1.2270411252975464, time 161.00937938690186\n",
      "Evaluate loss 1.226736307144165, time 161.60488104820251\n",
      "Evaluate loss 1.2358810901641846, time 162.20042538642883\n",
      "Evaluate loss 1.232383131980896, time 162.79579830169678\n",
      "Evaluate loss 1.2323198318481445, time 163.39203000068665\n",
      "Evaluate loss 1.2445757389068604, time 163.98791766166687\n",
      "Evaluate loss 1.242177128791809, time 164.5836479663849\n",
      "Evaluate loss 1.2317122220993042, time 165.17898678779602\n",
      "Evaluate loss 1.2313278913497925, time 165.77438116073608\n",
      "Evaluate loss 1.225113868713379, time 166.3698341846466\n",
      "Evaluate loss 1.241668939590454, time 166.96583032608032\n",
      "Evaluate loss 1.240431308746338, time 167.56147027015686\n",
      "Evaluate loss 1.231321096420288, time 168.1577184200287\n",
      "Evaluate loss 1.236502766609192, time 168.75461983680725\n",
      "Evaluate loss 1.2333449125289917, time 169.35060453414917\n",
      "Evaluate loss 1.2466665506362915, time 169.94696354866028\n",
      "Evaluate loss 1.231926679611206, time 170.54288363456726\n",
      "Evaluate loss 1.2282264232635498, time 171.1387791633606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate loss 1.2219680547714233, time 171.73472356796265\n",
      "Evaluate loss 1.221449851989746, time 172.3310101032257\n",
      "Evaluate loss 1.222186803817749, time 172.92722511291504\n",
      "Evaluate loss 1.222793459892273, time 173.5236337184906\n",
      "Evaluate loss 1.2155845165252686, time 174.12065982818604\n",
      "Evaluate loss 1.2189505100250244, time 174.71727967262268\n",
      "Evaluate loss 1.212111234664917, time 175.3140890598297\n",
      "Evaluate loss 1.2237862348556519, time 175.91034722328186\n",
      "Evaluate loss 1.2107315063476562, time 176.50656700134277\n",
      "Evaluate loss 1.1951760053634644, time 177.10266184806824\n",
      "Evaluate loss 1.2285171747207642, time 177.69953393936157\n",
      "Evaluate loss 1.2207238674163818, time 178.2956600189209\n",
      "Evaluate loss 1.2230889797210693, time 178.8917531967163\n",
      "Evaluate loss 1.218405842781067, time 179.48787569999695\n",
      "Evaluate loss 1.234965205192566, time 180.08413290977478\n",
      "Evaluate loss 1.2288219928741455, time 180.68092370033264\n",
      "Evaluate loss 1.2301366329193115, time 181.27692794799805\n",
      "Evaluate loss 1.2313780784606934, time 181.87309885025024\n",
      "Evaluate loss 1.2039918899536133, time 182.4692153930664\n",
      "Evaluate loss 1.2251890897750854, time 183.06574082374573\n",
      "Evaluate loss 1.2297961711883545, time 183.6621754169464\n",
      "Evaluate loss 1.2100218534469604, time 184.25926876068115\n",
      "Evaluate loss 1.2107024192810059, time 184.85557222366333\n",
      "Evaluate loss 1.2306116819381714, time 185.4529972076416\n",
      "Evaluate loss 1.225175380706787, time 186.0492000579834\n",
      "Evaluate loss 1.2140800952911377, time 186.6458547115326\n",
      "Evaluate loss 1.2090760469436646, time 187.24188899993896\n",
      "Evaluate loss 1.2172529697418213, time 187.8383083343506\n",
      "Evaluate loss 1.2256633043289185, time 188.4350254535675\n",
      "Evaluate loss 1.2082808017730713, time 189.03123474121094\n",
      "Evaluate loss 1.2230745553970337, time 189.6271460056305\n",
      "Evaluate loss 1.2249255180358887, time 190.22435188293457\n",
      "Evaluate loss 1.2194138765335083, time 190.82098197937012\n",
      "Evaluate loss 1.221887230873108, time 191.4173939228058\n",
      "Evaluate loss 1.202904462814331, time 192.01390600204468\n",
      "Evaluate loss 1.220741868019104, time 192.61197710037231\n",
      "Evaluate loss 1.241758942604065, time 193.20875716209412\n",
      "Evaluate loss 1.2357434034347534, time 193.80653071403503\n",
      "Evaluate loss 1.22916579246521, time 194.40276741981506\n",
      "Evaluate loss 1.2448431253433228, time 195.0001254081726\n",
      "Evaluate loss 1.2304474115371704, time 195.5969626903534\n",
      "Evaluate loss 1.2364565134048462, time 196.19413828849792\n",
      "Evaluate loss 1.2156965732574463, time 196.7905716896057\n",
      "Evaluate loss 1.2323733568191528, time 197.3871922492981\n",
      "Evaluate loss 1.258497953414917, time 197.98366689682007\n",
      "Evaluate loss 1.2286431789398193, time 198.58016729354858\n",
      "Evaluate loss 1.2307182550430298, time 199.17682218551636\n",
      "Evaluate loss 1.2394698858261108, time 199.7727828025818\n",
      "Evaluate loss 1.2509620189666748, time 200.3689889907837\n",
      "Evaluate loss 1.2342523336410522, time 200.9655077457428\n",
      "Evaluate loss 1.2293658256530762, time 201.56182479858398\n",
      "Evaluate loss 1.2406938076019287, time 202.1577501296997\n",
      "Evaluate loss 1.214272141456604, time 202.75373888015747\n",
      "Evaluate loss 1.2393296957015991, time 203.35075283050537\n",
      "Evaluate loss 1.2269701957702637, time 203.94758653640747\n",
      "Evaluate loss 1.2317508459091187, time 204.54422521591187\n",
      "Evaluate loss 1.219887137413025, time 205.14059448242188\n",
      "Evaluate loss 1.2405381202697754, time 205.73691773414612\n",
      "Evaluate loss 1.2161158323287964, time 206.33316826820374\n",
      "Evaluate loss 1.2269642353057861, time 206.92950296401978\n",
      "Evaluate loss 1.225469946861267, time 207.52616095542908\n",
      "Evaluate loss 1.2168911695480347, time 208.12408018112183\n",
      "Evaluate loss 1.2166849374771118, time 208.72040057182312\n",
      "Evaluate loss 1.2198166847229004, time 209.31865215301514\n",
      "Evaluate loss 1.2377606630325317, time 209.9147117137909\n",
      "Evaluate loss 1.228398084640503, time 210.51147842407227\n",
      "Evaluate loss 1.247646689414978, time 211.10762286186218\n",
      "Evaluate loss 1.2365483045578003, time 211.70456624031067\n",
      "Evaluate loss 1.2350904941558838, time 212.30151772499084\n",
      "Evaluate loss 1.2268784046173096, time 212.89857959747314\n",
      "Evaluate loss 1.240761160850525, time 213.49471926689148\n",
      "Evaluate loss 1.2180241346359253, time 214.09095668792725\n",
      "Evaluate loss 1.213423490524292, time 214.68658542633057\n",
      "Evaluate loss 1.243145227432251, time 215.2836561203003\n",
      "Evaluate loss 1.2278467416763306, time 215.87975478172302\n",
      "Evaluate loss 1.2275513410568237, time 216.47673273086548\n",
      "Evaluate loss 1.2473796606063843, time 217.07281160354614\n",
      "Evaluate loss 1.231377363204956, time 217.6696059703827\n",
      "Evaluate loss 1.2353754043579102, time 218.26587295532227\n",
      "Evaluate loss 1.2454286813735962, time 218.86210680007935\n",
      "Evaluate loss 1.235421895980835, time 219.45915389060974\n",
      "Evaluate loss 1.2293543815612793, time 220.05596590042114\n",
      "Evaluate loss 1.224973440170288, time 220.6523311138153\n",
      "Evaluate loss 1.2240701913833618, time 221.24857473373413\n",
      "Evaluate loss 1.2305901050567627, time 221.84489560127258\n",
      "Evaluate loss 1.2316796779632568, time 222.44119691848755\n",
      "Evaluate loss 1.232369065284729, time 223.03857731819153\n",
      "Evaluate loss 1.2211928367614746, time 223.63458800315857\n",
      "Evaluate loss 1.2502717971801758, time 224.23110008239746\n",
      "Evaluate loss 1.2389212846755981, time 224.82766151428223\n",
      "Evaluate loss 1.2196379899978638, time 225.4237608909607\n",
      "Evaluate loss 1.2283235788345337, time 226.0209538936615\n",
      "Evaluate loss 1.2420798540115356, time 226.6169638633728\n",
      "Evaluate loss 1.2155365943908691, time 227.21368789672852\n",
      "Evaluate loss 1.2151271104812622, time 227.81025457382202\n",
      "Evaluate loss 1.2386857271194458, time 228.40728092193604\n",
      "Evaluate loss 1.2368274927139282, time 229.0038866996765\n",
      "Evaluate loss 1.2382398843765259, time 229.59983849525452\n",
      "Evaluate loss 1.212180733680725, time 230.19706845283508\n",
      "Evaluate loss 1.2278469800949097, time 230.79415369033813\n",
      "Evaluate loss 1.22482430934906, time 231.3914816379547\n",
      "Evaluate loss 1.2361629009246826, time 231.9890172481537\n",
      "Evaluate loss 1.2196781635284424, time 232.58702397346497\n",
      "Evaluate loss 1.2298038005828857, time 233.18317556381226\n",
      "Evaluate loss 1.2265353202819824, time 233.77982330322266\n",
      "Evaluate loss 1.2187206745147705, time 234.37652802467346\n",
      "Evaluate loss 1.2290258407592773, time 234.9725377559662\n",
      "Evaluate loss 1.2411010265350342, time 235.5697958469391\n",
      "Evaluate loss 1.214239478111267, time 236.16628766059875\n",
      "Evaluate loss 1.225441336631775, time 236.76342058181763\n",
      "Evaluate loss 1.224396824836731, time 237.3598611354828\n",
      "Evaluate loss 1.2321444749832153, time 237.95613026618958\n",
      "Evaluate loss 1.2300548553466797, time 238.55254936218262\n",
      "Evaluate loss 1.2279382944107056, time 239.14929723739624\n",
      "Evaluate loss 1.2315483093261719, time 239.7455403804779\n",
      "Evaluate loss 1.226953148841858, time 240.3424689769745\n",
      "Evaluate loss 1.222334861755371, time 240.93914890289307\n",
      "Evaluate loss 1.2296440601348877, time 241.5353446006775\n",
      "Evaluate loss 1.218272089958191, time 242.13199472427368\n",
      "Evaluate loss 1.2290592193603516, time 242.7289412021637\n",
      "Evaluate loss 1.2349902391433716, time 243.32570600509644\n",
      "Evaluate loss 1.2330131530761719, time 243.9218611717224\n",
      "Evaluate loss 1.2425578832626343, time 244.5179877281189\n",
      "Evaluate loss 1.2293269634246826, time 245.11503505706787\n",
      "Evaluate loss 1.2340891361236572, time 245.71124982833862\n",
      "Evaluate loss 1.2253024578094482, time 246.30772042274475\n",
      "Evaluate loss 1.221808910369873, time 246.9038107395172\n",
      "Evaluate loss 1.2277504205703735, time 247.4999487400055\n",
      "Evaluate loss 1.2311782836914062, time 248.0962519645691\n",
      "Evaluate loss 1.2335293292999268, time 248.69277596473694\n",
      "Evaluate loss 1.217811942100525, time 249.2889368534088\n",
      "Evaluate loss 1.215869426727295, time 249.88498520851135\n",
      "Evaluate loss 1.2074569463729858, time 250.481791973114\n",
      "Evaluate loss 1.2139350175857544, time 251.07842326164246\n",
      "Evaluate loss 1.2182670831680298, time 251.675475358963\n",
      "Evaluate loss 1.2201008796691895, time 252.27180886268616\n",
      "Evaluate loss 1.2308651208877563, time 252.86794686317444\n",
      "Evaluate loss 1.2214505672454834, time 253.46423816680908\n",
      "Evaluate loss 1.2211382389068604, time 254.06047415733337\n",
      "Evaluate loss 1.2302225828170776, time 254.6584107875824\n",
      "Evaluate loss 1.2276562452316284, time 255.25574922561646\n",
      "Evaluate loss 1.2149617671966553, time 255.8522789478302\n",
      "Evaluate loss 1.227137804031372, time 256.44851064682007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate loss 1.234251856803894, time 257.044960975647\n",
      "Evaluate loss 1.2242413759231567, time 257.6411590576172\n",
      "Evaluate loss 1.23050856590271, time 258.23758268356323\n",
      "Evaluate loss 1.2247917652130127, time 258.83385586738586\n",
      "Evaluate loss 1.2323541641235352, time 259.431676864624\n",
      "Evaluate loss 1.22425377368927, time 260.0280261039734\n",
      "Evaluate loss 1.2221533060073853, time 260.6242094039917\n",
      "Evaluate loss 1.2173981666564941, time 261.2205128669739\n",
      "Evaluate loss 1.2247272729873657, time 261.81762194633484\n",
      "Evaluate loss 1.224429965019226, time 262.4139702320099\n",
      "Evaluate loss 1.230566143989563, time 263.011438369751\n",
      "Evaluate loss 1.2167742252349854, time 263.6078293323517\n",
      "Evaluate loss 1.2372338771820068, time 264.20400643348694\n",
      "Evaluate loss 1.2265651226043701, time 264.80083560943604\n",
      "Evaluate loss 1.2297868728637695, time 265.39694356918335\n",
      "Evaluate loss 1.238741159439087, time 265.99284195899963\n",
      "Evaluate loss 1.2210954427719116, time 266.5895662307739\n",
      "Evaluate loss 1.2220721244812012, time 267.1857485771179\n",
      "Evaluate loss 1.2387971878051758, time 267.7830946445465\n",
      "Evaluate loss 1.2359224557876587, time 268.3796741962433\n",
      "Evaluate loss 1.2245596647262573, time 268.9762587547302\n",
      "Evaluate loss 1.2233494520187378, time 269.5726363658905\n",
      "Evaluate loss 1.2352378368377686, time 270.16915225982666\n",
      "Evaluate loss 1.2337454557418823, time 270.76557898521423\n",
      "Evaluate loss 1.212874412536621, time 271.3628785610199\n",
      "Evaluate loss 1.2183815240859985, time 271.9591488838196\n",
      "Evaluate loss 1.2246211767196655, time 272.5552065372467\n",
      "Evaluate loss 1.2171090841293335, time 273.15154480934143\n",
      "Evaluate loss 1.1987911462783813, time 273.74849915504456\n",
      "Evaluate loss 1.2054556608200073, time 274.3466548919678\n",
      "Evaluate loss 1.2217408418655396, time 274.94263648986816\n",
      "Evaluate loss 1.2171146869659424, time 275.5385444164276\n",
      "Evaluate loss 1.2219326496124268, time 276.1350131034851\n",
      "Evaluate loss 1.2168246507644653, time 276.73101568222046\n",
      "Evaluate loss 1.218845248222351, time 277.32844853401184\n",
      "Evaluate loss 1.2193210124969482, time 277.92490816116333\n",
      "Evaluate loss 1.2007702589035034, time 278.52142667770386\n",
      "Evaluate loss 1.2226142883300781, time 279.11820554733276\n",
      "Evaluate loss 1.217517375946045, time 279.7144763469696\n",
      "Evaluate loss 1.20919668674469, time 280.3110065460205\n",
      "Evaluate loss 1.2143789529800415, time 280.9094114303589\n",
      "Evaluate loss 1.2190468311309814, time 281.506254196167\n",
      "Evaluate loss 1.2139472961425781, time 282.10270953178406\n",
      "Evaluate loss 1.2100272178649902, time 282.69988775253296\n",
      "Evaluate loss 1.2192093133926392, time 283.2962863445282\n",
      "Evaluate loss 1.2351551055908203, time 283.89290046691895\n",
      "Evaluate loss 1.2159101963043213, time 284.072274684906\n",
      "Evaluate total loss 1.2240846543540747, time 284.0723261833191, avg.time\n"
     ]
    }
   ],
   "source": [
    "evaluate_step(test_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
